
\chapter{Self-Adjusting Programs}
\label{ch:self_adjusting}

When changing the input set of a self-adjusting program, the program has to adjust the internal state and the program output to match the new input. This requires that the internal state of the program has to be known before propagating input changes. The first execution of the program, called \textit{Initial Run} executes the whole program with the given input and captures the program state. The state consits of all data and control dependencies of the execution and all functions called during program executing, including their referencing environment. 
For each successive input update, a so called \textit{Change Propagation Algorithm} consults the program state, finds the parts of the program to reexecute and reexecutes them, updating the output and the state.  
This chapter explains which datastructures are used to hold the state and how the mechanisms of change propagation work. The principles of self-adjusting programs using DDGs and memoization were first described by Acar in \cite{Acar2005thesis}, including a theoretical analysis. 

As already mentioned, the a self-adjusting program has no way to modify the output directly. Instead, the change propagation is handled by the platform or language which provides us self-adjusting computation. For our simple map example from section \ref{sec:simple_example}, this means that we do no longer have to write specialized code to find the application of the mapping function and update the output. When using self-adjusting computation, we would just write the program in a suitable language, as we would write a non-incremental version, and the underlying platform will take care about dynamizing the program.

\section{Dynamic Dependence Graphs}

A \textit{Dynamic Dependence Graph} (\textit{DDG}) is a directed, acyclic graph. The nodes of this graph represent the function calls in the program. These nodes also hold the state of the referencing environment, or, in other words, including all parameters of the function and all variables bound from outer scopes. The edges represent control dependencies and data dependencies. 

%Simple Map DDG
\begin{figure}
\begin{center}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,
  thick,call node/.style={circle,fill=white!20,draw},
  data node/.style={rectangle,fill=white!20,draw}]

  \node[data node] (0) {0};
  \node[data node] (1) [right of=0] {1};
  \node[data node] (2) [right of=1] {2};
  \node[data node] (3) [right of=2] {3};

  \node[call node] (4) [below of=0, ultra thick] {map(0)};
  \node[call node] (5) [below of=1] {map(1)};
  \node[call node] (6) [below of=2] {map(2)};
  \node[call node] (7) [below of=3] {map(3)};

  \node[data node] (8) [below of=4] {0};
  \node[data node] (9) [below of=5] {2};
  \node[data node] (10) [below of=6] {4};
  \node[data node] (11) [below of=7] {6};

  %Input next edges
  \path[thin]
    (0) edge node [above] {next} (1)
    (1) edge node [above] {next} (2)
    (2) edge node [above] {next} (3)
    (8) edge node [above] {next} (9)
    (9) edge node [above] {next} (10)
    (10) edge node [above] {next} (11);

  %Read/Write dependencies
  \path[dashed]
    (0) edge node [left] {data} (4)
    (1) edge node [left] {data} (5)
    (2) edge node [left] {data} (6)
    (3) edge node [left] {data} (7)
    (4) edge node [left] {data} (8)
    (5) edge node [left] {data} (9)
    (6) edge node [left] {data} (10)
    (7) edge node [left] {data} (11);

  %Controld ependencies
  \path[ultra thick]
    (4) edge node [above] {call} (5)
    (5) edge node [above] {call} (6)
    (6) edge node [above] {call} (7);
\end{tikzpicture}
\end{center}
\caption{The DDG of a map operation with four list elements}
\label{fig:map_ddg}
\end{figure}

Figure \ref{fig:map_ddg} shows the DDG of a single execution of an recursive implementation of our map-sample from Section \ref{sec:simple_example} with four input elements. The program consists of a single function $map$ which reads the element, applies the mapping function $f$ to the value of the element and calls $map$ again with the elements successor. The square nodes at the top denote elements of the input list. The square nodes at the bottom represent elements of the output list. The round nodes represent function calls to the function $map$. 

The edges labeled with \textit{next} indicate the next pointers of the input and output list. Those edges are not a part of the DDG. The dashed edges labeled with \textit{data} correspond to data dependencies. Note that there is a \textit{read} dependency between from each input element to the corresponding application of the mapping function $f$ and from there to the corresponding output element. The thick edges labelled with \textit{call} correspond to the control dependencies. Those control dependencies reflect, that $map$ is recursively called for each element. 

On an input change, the change propagation would now track all outgoing dependencies from the changed input values and re-execute the nodes which are dependant on the changed value. During this process, it is possible that values which affect other parts of the program are updated. Those values are then queued for change propagation. Also, since new function calls might be placed during change propagation, the program structure and therefore the DDG can change.  

%Simple Map DDG with change propagation
\begin{figure}
\begin{center}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2cm,
  thick,call node/.style={circle,fill=white!20,draw},
  data node/.style={rectangle,fill=white!20,draw}]

  \node[data node] (0) {0};
  \node[data node] (1) [right of=0] {1};
  \node[data node] (2) [right of=1, style={draw=red}] {6};
  \node[data node] (3) [right of=2] {3};

  \node[call node] (4) [below of=0, ultra thick] {map(0)};
  \node[call node] (5) [below of=1] {map(1)};
  \node[call node] (6) [below of=2, style={draw=red}] {map(6)};
  \node[call node] (7) [below of=3, style={draw=red}] {map(3)};

  \node[data node] (8) [below of=4] {0};
  \node[data node] (9) [below of=5] {2};
  \node[data node] (10) [below of=6, style={draw=red}] {12};
  \node[data node] (11) [below of=7, style={draw=red}] {6};

  %Input next edges
  \path[thin]
    (0) edge node [above] {} (1)
    (1) edge node [above] {} (2)
    (2) edge node [above] {} (3)
    (8) edge node [above] {} (9)
    (9) edge node [above] {} (10)
    (10) edge node [above] {} (11);

  %Read/Write dependencies
  \path[dashed]
    (0) edge node [left] {} (4)
    (1) edge node [left] {} (5)
    (3) edge node [left] {} (7)
    (4) edge node [left] {} (8)
    (5) edge node [left] {} (9);

  \path[red, dashed, ultra thick]
    (2) edge node [left] {} (6)
    (6) edge node [left] {} (10)
    (7) edge node [left] {} (11);

  %Controld ependencies
  \path[ultra thick]
    (4) edge node [above] {} (5)
    (5) edge node [above] {} (6);

  \path[red, ultra thick]
    (6) edge node [above] {} (7);
\end{tikzpicture}
\end{center}
\caption{The DDG of map after change propagation}
\label{fig:map_change_ddg}
\end{figure}

Figure \ref{fig:map_change_ddg} shows the DDG of the simple map example after changing the third input value from $2$ to $6$. The highlighted edges have been traversed, and the highlighted nodes were therefore updated. It can be seen that the node $map(3)$ has been re-executed, even if that re-execution is not necessary. For a long list, this would lead to unnecessarily re-evaluating a lot of function calls. The result is linear time complexity in the worst case, even if only a single element has been updated. To circumvent this shortcoming, another technique is necessarry: \textit{Memoization}.

\section{Memoization}

\textit{Memoization}, or also called \textit{Function Caching} is the task of storing results of expensive computations. Basically, it is possible to remember the input parameters and the result for a given function. Then, the parameters of repetive calls can be compared to the stored parameters. If the parameters match, the result can be re-used. Such a match is called \textit{memo match}.

In combination with DDGs, this means that we can re-use whole subtrees of the DDG during change propagation. Figure \ref{fig:map_change_ddg_memo} illustrates the change propagation process using a \textit{memoized DDG}. Again, the change propagation algorithm re-evaluates functions dependent on the changed input. When the algorithm reaches the call to $map(3)$ however, the parameters of the call match the previous call. The algorithm therefore skips re-executing the call to $map(3)$ and re-uses the subtree. With this technique, the update of a constant number of elements finishes in costant time, for lists of any length. 
The memo match is highlighted in green. 

%Simple Map DDG with change propagation
\begin{figure}
\begin{center}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2cm,
  thick,call node/.style={circle,fill=white!20,draw},
  data node/.style={rectangle,fill=white!20,draw}]

  \node[data node] (0) {0};
  \node[data node] (1) [right of=0] {1};
  \node[data node] (2) [right of=1, style={draw=red}] {6};
  \node[data node] (3) [right of=2] {3};

  \node[call node] (4) [below of=0, ultra thick] {map(0)};
  \node[call node] (5) [below of=1] {map(1)};
  \node[call node] (6) [below of=2, style={draw=red}] {map(6)};
  \node[call node] (7) [below of=3, style={draw=green}] {map(3)};

  \node[data node] (8) [below of=4] {0};
  \node[data node] (9) [below of=5] {2};
  \node[data node] (10) [below of=6, style={draw=red}] {12};
  \node[data node] (11) [below of=7] {6};

  %Input next edges
  \path[thin]
    (0) edge node [above] {} (1)
    (1) edge node [above] {} (2)
    (2) edge node [above] {} (3)
    (8) edge node [above] {} (9)
    (9) edge node [above] {} (10)
    (10) edge node [above] {} (11);

  %Read/Write dependencies
  \path[dashed]
    (0) edge node [left] {} (4)
    (1) edge node [left] {} (5)
    (3) edge node [left] {} (7)
    (4) edge node [left] {} (8)
    (5) edge node [left] {} (9)
    (7) edge node [left] {} (11);

  \path[red, dashed, ultra thick]
    (2) edge node [left] {} (6)
    (6) edge node [left] {} (10);

  %Control ependencies
  \path[ultra thick]
    (4) edge node [above] {} (5)
    (5) edge node [above] {} (6);

  \path[red, ultra thick]
    (6) edge node [above] {} (7);
\end{tikzpicture}
\end{center}
\caption{The memoized DDG of map after change propagation}
\label{fig:map_change_ddg_memo}
\end{figure}

It should be noted that for memoization to work, all functions are required to be \textit{pure} or \textit{side-effect free}. 

\section{A more complex example}

As already mentioned, the example from Section \ref{ch:use_cases} does not have dependencies between different function calls, and is therefore straight forward to dynamize. We therefore complete this section with a more complex example. 

Many programming languages feature an operator which is capable of combining all elements with some operator, until there is only a single result left. This operation is known as \textit{reduce}, \textit{combine} or \textit{fold}.  

\subsection{Naive Approach}

Again, we consider a linked list $A = (a_1, ..., a_n)$ with list nodes $a_i = (v_i, n_i)$, each having a value and a next node, and a combining function $f(v_x, v_y) = r$, where $v_x$ and $v_y$ are input values and $r$ denotes the result. $v_x$, $v_y$ and $r$ are of the same type. Further, we consider the list nodes immutable for our program only. Changes can be done from outside the program when the input changes.

A classic reduce operation would read the first two nodes $a_1$ and $a_2$ in the list, combine the value $v_1$ with the calue $v_2$ and remember the result $f(v_1, v_2) = r_1$. Then, the next node $a_3$ is read, the value is combined with the previous result $r_1$ and the result $r_2$ is stored again. In general we can say that for each node $a_i$ we combine the value of the node $v_i$ with the previous result $r_{i-1}$. The last result $r_{n-1}$ is the final result of the operator. 

If we now track dependencies for this operation, each result $r_i$ depends on the previous result $r_{i - 1}$ or the initial value $r_0$. Each result $r_i$ also depends on the value $v_i$ of the corresponding node $a_i$. In other words, the fold operation for each node $a_j, j > 1$ depends on the successors of the respective node $a_{j - k}, 0 < k < j$.

The dependency graph of such an implementation is shown in figure \ref{fig:naive_reduce}. 

%Folfl DDG
\begin{figure}
\begin{center}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2cm,
  thick,call node/.style={circle,fill=white!20,draw},
  data node/.style={rectangle,fill=white!20,draw}]

  \node[data node] (0) {0};
  \node[data node] (1) [right of=0] {1};
  \node[data node] (2) [right of=1] {2};
  \node[data node] (3) [right of=2] {3};
  \node[data node] (4) [right of=3] {4};
  \node[data node] (5) [right of=4] {5};
  \node[data node] (6) [right of=5] {6};
  \node[data node] (7) [right of=6] {7};

  \node[call node] (10) [below of=1, ultra thick] {f(0, 1)};
  \node[call node] (11) [below of=2] {f(1, 2)};
  \node[call node] (12) [below of=3] {f(3, 3)};
  \node[call node] (13) [below of=4] {f(6, 4)};
  \node[call node] (14) [below of=5] {f(10, 5)};
  \node[call node] (15) [below of=6] {f(15, 6)};
  \node[call node] (16) [below of=7] {f(21, 7)};

  \node[data node] (18) [below of=16] {28};

  %Input next edges
  \path[thin]
    (0) edge node [above] {} (1)
    (1) edge node [above] {} (2)
    (2) edge node [above] {} (3)
    (3) edge node [above] {} (4)
    (4) edge node [above] {} (5)
    (5) edge node [above] {} (6)
    (6) edge node [above] {} (7);

  %Read/Write dependencies
  \path[dashed]
    (0) edge node [left] {} (10)
    (1) edge node [left] {} (10)
    (2) edge node [left] {} (11)
    (3) edge node [left] {} (12)
    (4) edge node [left] {} (13)
    (5) edge node [left] {} (14)
    (6) edge node [left] {} (15)
    (7) edge node [left] {} (16)

    (16) edge node [left] {} (18)

    (10) edge [bend right] node {} (11)
    (11) edge [bend right] node {} (12)
    (12) edge [bend right] node {} (13)
    (13) edge [bend right] node {} (14)
    (14) edge [bend right] node {} (15)
    (15) edge [bend right] node {} (16);

  %Control ependencies
  \path[ultra thick]
    (10) edge node [left] {} (11)
    (11) edge node [left] {} (12)
    (12) edge node [left] {} (13)
    (13) edge node [left] {} (14)
    (14) edge node [left] {} (15)
    (15) edge node [left] {} (16);

\end{tikzpicture}
\end{center}
\caption{DDG of a naive reduce operation}
\label{fig:naive_reduce}
\end{figure}

If now a change to the first node in the input happens, a change propagation always has to re-execute the fold operation for the whole list, since every single intermediate result and the final result depend on the first node of the list. For this case, the asymptotic complexity of change propagation lies within $O(n)$ whereas $n$ donates the input since. This expected bound also holds for the average case. The algorithm therefore updates in $O(n)$, which is not better than the expected time for a re-execution. 

Note that memoization is of no help here, since there are data dependencies to the result of the previous reduction, involving all predecessors of the node.

\subsection{Tree-Like Implementation}

Combining all elements in a list can also be done in a \textit{tree-like} manner. This means that instead of combining all elements from the beginning to the end of the list, elements are pairwise combined, until the list is empty. 

More formally, for our input list $A_1 = (a_{1, 1}, a_{1, 2}, ..., a_{1, n})$, we combine each element with an odd index $a_i$ with its successor $a_{i + 1}$, and concatenate the results to a new list $A_2 = (a_{(2, 1)}, ..., a_{2, k})$. If the input list has an odd number of elements, the last element of the input list is added to the result list. Then, we repeat this operation with the result list, until there is only a single element left.

During each reduction step, half of the list is eliminated. That implies that for $n$ input elements, the count of reduction steps lies within the complexity class $O(log(n))$. If we consult the data dependencies, each element in the list $A_j, j > 1$ directly depends on two elements, which are contained in the list $A_{j - i}$. Since there are only $O(log(n))$ reduction steps and therefore only $O(log(n))$ intermediate lists, the length of the dependency path for each element, including the final result, lies within $O(log(n))$. It is therefore possible to propagate the change of a constant number of values in $(O(log(n))$. 

%Tree-Fold DDG
\begin{figure}
\begin{center}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2cm,
  thick,call node/.style={circle,fill=white!20,draw},
  data node/.style={rectangle,fill=white!20,draw}]

  \node[data node] (0) {0};
  \node[data node] (1) [right of=0] {1};
  \node[data node] (2) [right of=1] {2};
  \node[data node] (3) [right of=2] {3};
  \node[data node] (4) [right of=3] {4};
  \node[data node] (5) [right of=4] {5};
  \node[data node] (6) [right of=5] {6};
  \node[data node] (7) [right of=6] {7};

  \node[call node] (10) [below of=0, ultra thick] {f(0, 1)};
  \node[call node] (11) [below of=2] {f(2, 3)};
  \node[call node] (12) [below of=4] {f(4, 5)};
  \node[call node] (13) [below of=6] {f(6, 7)};

  \node[call node] (14) [below of=10] {f(1, 5)};
  \node[call node] (15) [below of=12] {f(9, 13)};

  \node[call node] (16) [below of=14] {f(6, 22)};

  \node[data node] (18) [below of=16] {28};

  %Input next edges
  \path[thin]
    (0) edge node [above] {} (1)
    (1) edge node [above] {} (2)
    (2) edge node [above] {} (3)
    (3) edge node [above] {} (4)
    (4) edge node [above] {} (5)
    (5) edge node [above] {} (6)
    (6) edge node [above] {} (7);

  %Read/Write dependencies
  \path[dashed]
    (0) edge node [left] {} (10)
    (1) edge node [left] {} (10)
    (2) edge node [left] {} (11)
    (3) edge node [left] {} (11)
    (4) edge node [left] {} (12)
    (5) edge node [left] {} (12)
    (6) edge node [left] {} (13)
    (7) edge node [left] {} (13)

    (10) edge node [left] {} (14)
    (11) edge node [left] {} (14)
    (12) edge node [left] {} (15)
    (13) edge node [left] {} (15)

    (14) edge node [left] {} (16)
    (15) edge node [left] {} (16)

    (16) edge node [left] {} (18);

  %Control ependencies
  \path[ultra thick]
    (10) edge node [left] {} (11)
    (11) edge node [left] {} (12)
    (12) edge node [left] {} (13)
    (13) edge node [left] {} (14)
    (14) edge node [left] {} (15)
    (15) edge node [left] {} (16);

\end{tikzpicture}
\end{center}
\caption{DDG of a tree-like reduce operation}
\label{fig:tree_reduce}
\end{figure}

Figure \ref{fig:tree_reduce} shows the DDG of the tree-like reduce operation. It can be recognized that there are no data dependencies, which connect all nodes, like before. The control dependencies, while still existant, do not bear any problems, since we can utilize memoization to stop change propagation between nodes which feature no data dependencies. 

While the upper bound of $O(log(n))$ holds for updates of values, this bound is not guaranteed for inserts or deletions of whole list elements from the input list. If we remove or add a single element, we change the index of all successive elements. Since we combine each odd element with its successor, changing the index of elements also changes which elements are combined. If now, for example, the first element is removed, we have to re-evaluate the reduction of all successors, which equals an re-evaluation for the entire list. 
It is possible to circumvent this drawback by using a slightly more complicated approach which involves randomization. This approach is discussed in section \ref{ch:use_cases}. 

\chapter{The TBD Platform}
\label{ch:tbd_platform}

\section{Program interface}

\chapter{Calculating Trace Distance}
\label{ch:implementation}

\section{Node Tags}

\section{Trace Distance}

\chapter{Implementation}
\label{ch:impl}

\section{Architecture}

\section{Data Collection}

\subsection{Usage of Scala Macros}

\chapter{Use Cases}
\label{ch:use_cases}

\chapter{Discussion}
\label{ch:discussion}