
\chapter{Self-Adjusting Programs}
\label{ch:self_adjusting}

When changing the input set of a self-adjusting program, the program has to adjust the internal state and the program output to match the new input. This requires that the internal state of the program has to be known before propagating input changes. The first execution of the program, called \textit{initial run} executes the whole program with the given input and captures the program state. The state consists of all data and control dependencies of the execution and all functions called during program executing, including their referencing environment. 
For each successive input update, a so called \textit{change propagation algorithm} consults the program state, finds the parts of the program to re-execute and re-executes them, updating the output and the state.  
This chapter explains which data structures are used to hold the state and how the mechanisms of change propagation work. The principles of self-adjusting programs using DDGs and memoization were first described by Acar in \cite{Acar2005thesis}, including a theoretical analysis. 

As already mentioned, the a self-adjusting program has no way to modify the output directly. Instead, the change propagation is handled by the platform or language that provides us self-adjusting computation. For our simple map example from section \ref{sec:simple_example}, this means that we do no longer have to write specialized code to find the application of the mapping function and update the output. When using self-adjusting computation, we would just write the program in a suitable language, as we would write a non-incremental version, and the underlying platform will take care about dynamizing the program.

\section{Dynamic Dependence Graphs}
\label{sec:ddg}

A \textit{dynamic dependence graph} (\textit{DDG}) is a directed, acyclic graph. The nodes of this graph represent the function calls in the program. These nodes also hold the state of the referencing environment, or, in other words, including all parameters of the function and all variables bound from outer scopes. The edges represent control dependencies and data dependencies. 

%Simple Map DDG
\begin{figure}
\begin{center}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,
  thick,call node/.style={circle,fill=white!20,draw},
  data node/.style={rectangle,fill=white!20,draw}]

  \node[data node] (0) {0};
  \node[data node] (1) [right of=0] {1};
  \node[data node] (2) [right of=1] {2};
  \node[data node] (3) [right of=2] {3};

  \node[call node] (4) [below of=0, ultra thick] {f(0)};
  \node[call node] (5) [below of=1] {f(1)};
  \node[call node] (6) [below of=2] {f(2)};
  \node[call node] (7) [below of=3] {f(3)};

  \node[data node] (8) [below of=4] {0};
  \node[data node] (9) [below of=5] {2};
  \node[data node] (10) [below of=6] {4};
  \node[data node] (11) [below of=7] {6};

  %Input next edges
  \path[thin]
    (0) edge node [above] {next} (1)
    (1) edge node [above] {next} (2)
    (2) edge node [above] {next} (3)
    (8) edge node [above] {next} (9)
    (9) edge node [above] {next} (10)
    (10) edge node [above] {next} (11);

  %Read/Write dependencies
  \path[dashed]
    (0) edge node [left] {data} (4)
    (1) edge node [left] {data} (5)
    (2) edge node [left] {data} (6)
    (3) edge node [left] {data} (7)
    (4) edge node [left] {data} (8)
    (5) edge node [left] {data} (9)
    (6) edge node [left] {data} (10)
    (7) edge node [left] {data} (11);

  %Controld ependencies
  \path[ultra thick]
    (4) edge node [above] {call} (5)
    (5) edge node [above] {call} (6)
    (6) edge node [above] {call} (7);
\end{tikzpicture}
\end{center}
\caption{The DDG of a map operation with four list elements}
\label{fig:map_ddg}
\end{figure}

Figure \ref{fig:map_ddg} shows the DDG of a single execution of an recursive implementation of our map-sample from Section \ref{sec:simple_example} with four input elements. The program consists of a single function $f$ that reads the element, applies the mapping function to the value of the element and calls $f$ again with the elements successor. The square nodes at the top denote elements of the input list. The square nodes at the bottom represent elements of the output list. The round nodes represent function calls to the function $f$. 

The thin edges labeled with \textit{next} indicate the next pointers of the input and output list's nodes. Those edges are not a part of the DDG. The dashed edges labeled with \textit{data} correspond to data dependencies. Note that there is a \textit{read} dependency from each input element to the corresponding application of the mapping function $f$ and from there to the corresponding output element. The thick edges labeled with \textit{call} correspond to the control dependencies. Those control dependencies reflect that $f$ is recursively called for each element. 

On an input change, the change propagation would now track all outgoing dependencies from the changed input values and re-execute the nodes that are dependent on the changed value. During this process, it is possible that values that affect other parts of the program are updated. Those values are then queued for change propagation. Also, since new function calls might be placed during change propagation, the program structure and therefore the DDG can change.  

%Simple Map DDG with change propagation
\begin{figure}
\begin{center}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2cm,
  thick,call node/.style={circle,fill=white!20,draw},
  data node/.style={rectangle,fill=white!20,draw}]

  \node[data node] (0) {0};
  \node[data node] (1) [right of=0, style={draw=red}] {4};
  \node[data node] (2) [right of=1] {2};
  \node[data node] (3) [right of=2] {3};

  \node[call node] (4) [below of=0, ultra thick] {f(0)};
  \node[call node] (5) [below of=1, style={draw=red}] {f(4)};
  \node[call node] (6) [below of=2, style={draw=red}] {f(2)};
  \node[call node] (7) [below of=3, style={draw=red}] {f(3)};

  \node[data node] (8) [below of=4] {0};
  \node[data node] (9) [below of=5, style={draw=red}] {8};
  \node[data node] (10) [below of=6, style={draw=red}] {4};
  \node[data node] (11) [below of=7, style={draw=red}] {6};

  %Input next edges
  \path[thin]
    (0) edge node [above] {} (1)
    (1) edge node [above] {} (2)
    (2) edge node [above] {} (3)
    (8) edge node [above] {} (9)
    (9) edge node [above] {} (10)
    (10) edge node [above] {} (11);

  %Read/Write dependencies
  \path[dashed]
    (0) edge node [left] {} (4)
    (3) edge node [left] {} (7)
    (4) edge node [left] {} (8);

  \path[red, dashed, ultra thick]
    (1) edge node [left] {} (5)
    (2) edge node [left] {} (6)
    (5) edge node [left] {} (9)
    (6) edge node [left] {} (10)
    (7) edge node [left] {} (11);

  %Controld ependencies
  \path[ultra thick]
    (4) edge node [above] {} (5)
    (5) edge node [above] {} (6);

  \path[red, ultra thick]
    (6) edge node [above] {} (7);
\end{tikzpicture}
\end{center}
\caption{The DDG of map after change propagation}
\label{fig:map_change_ddg}
\end{figure}

Figure \ref{fig:map_change_ddg} shows the DDG of the simple map example after changing the second input value from $1$ to $4$. The highlighted edges have been traversed, and the highlighted nodes were therefore updated. It can be seen that the nodes $f(2)$ and $f(3)$ have been re-executed. The re-execution was not necessary, since the parameters and therefore the result of the function call were the same before change propagation. For a long list, this would lead to unnecessarily re-evaluating a lot of function calls. The outcome of such a behavior is linear time complexity in the worst case, even if only a single element has been updated. To circumvent this shortcoming, another technique is required: Memoization.

\section{Memoization}

\textit{Memoization}, or also called \textit{function caching} is the task of storing results of expensive computations. Basically, it is possible to remember the input parameters and the result for a given function. Then, the parameters of repetitive calls can be compared to the stored parameters. If the parameters match, the result can be re-used. Such a match is called \textit{memo match}.

In combination with DDGs, this means that we can re-use whole subtrees of the DDG during change propagation. Figure \ref{fig:map_change_ddg_memo} illustrates the change propagation process using a memoized DDG. Again, the change propagation algorithm re-evaluates functions dependent on the changed input. When the algorithm reaches the call to $f(2)$ however, the parameters of the call match the previous call. The algorithm therefore skips re-executing the call to $f(2)$ and re-uses the subtree. With this technique, the update of a constant number of elements finishes in constant time, for lists of any length. 
The memo match is highlighted in green. 

%Simple Map DDG with change propagation
\begin{figure}
\begin{center}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2cm,
  thick,call node/.style={circle,fill=white!20,draw},
  data node/.style={rectangle,fill=white!20,draw}]

  \node[data node] (0) {0};
  \node[data node] (1) [right of=0, style={draw=red}] {4};
  \node[data node] (2) [right of=1] {2};
  \node[data node] (3) [right of=2] {3};

  \node[call node] (4) [below of=0, ultra thick] {f(0)};
  \node[call node] (5) [below of=1, style={draw=red}] {f(4)};
  \node[call node] (6) [below of=2, style={draw=green}] {f(2)};
  \node[call node] (7) [below of=3] {f(3)};

  \node[data node] (8) [below of=4] {0};
  \node[data node] (9) [below of=5, style={draw=red}] {8};
  \node[data node] (10) [below of=6] {4};
  \node[data node] (11) [below of=7] {6};

  %Input next edges
  \path[thin]
    (0) edge node [above] {} (1)
    (1) edge node [above] {} (2)
    (2) edge node [above] {} (3)
    (8) edge node [above] {} (9)
    (9) edge node [above] {} (10)
    (10) edge node [above] {} (11);

  %Read/Write dependencies
  \path[dashed]
    (0) edge node [left] {} (4)
    (2) edge node [left] {} (6)
    (3) edge node [left] {} (7)
    (4) edge node [left] {} (8)
    (6) edge node [left] {} (10)
    (7) edge node [left] {} (11);

  \path[red, dashed, ultra thick]
    (1) edge node [left] {} (5)
    (5) edge node [left] {} (9);

  %Control ependencies
  \path[ultra thick]
    (4) edge node [above] {} (5)
    (6) edge node [above] {} (7);

  \path[red, ultra thick]
    (5) edge node [above] {} (6);
\end{tikzpicture}
\end{center}
\caption{The memoized DDG of map after change propagation}
\label{fig:map_change_ddg_memo}
\end{figure}

It should be noted that for memoization to work, all functions are required to be \textit{pure} or \textit{side-effect free}. 

\section{Keyed Allocation}
\label{sec:keyed_alloc}

When a memory in a self-adjusting program is allocated, and this allocation call is re-executed during change propagation, a new memory location will be allocated. This behavior is desired in general, since we do not want to overwrite memory during change propagation. However, the memory location might be read again inside the program or even used for memoization. In this case, changing the memory location will require updating all references through the program to the new location during change propagation. This behavior can cost a lot of performance. 

The solution for this problem is keyed allocation. \textit{Keyed allocation} is the task of allocating memory depending on a key, passed to the allocation call. If a key matches the key used in a previous allocation, the old memory location is re-used. The old value will be overwritten. Therefore, the change propagation algorithm can use the DDG to identify all parts which have to be updated, but it is not needed to update all references. 

To illustrate the need for this concept, consider a list reverse operation that is implemented recursively. This operation takes an input list $A = (a_1, ..., a_n)$ and creates the reversed output list $A_r = (a_n, ..., a_1)$. Let $r$ be our recursive reverse operation. Whenever $r$ is called, the pointer to the next element is read. Then, a new element is allocated, with the value of the current element as value and the head of the accumulator list as next element. The new element now is the new head of the accumulator list. Then, $r$ is called with the next element and the new head of the accumulator list. If the end of the list is reached, $r$ is not called again. The first call of $r$ has the head of the input list and a null reference as parameter. 

The DDG of such a function is shown in Figure \ref{fig:reverse_ddg}. The recursive reverse call is denoted as $r$, with the current element as first, and the head of the accumulator list as second parameter. For clarity, the list elements are denoted as their respective value. The data dependencies between the calls arise from the next-relation of the list elements and the allocated memory location for the accumulator list. 


%Reverse DDG
\begin{figure}
\begin{center}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2cm,
  thick,call node/.style={circle,fill=white!20,draw},
  data node/.style={rectangle,fill=white!20,draw}]

  \node[data node] (0) {0};
  \node[data node] (1) [right of=0] {1};
  \node[data node] (2) [right of=1] {2};
  \node[data node] (3) [right of=2] {3};
  \node[data node] (4) [right of=3] {4};
  \node[data node] (5) [right of=4] {5};
  \node[data node] (6) [right of=5] {6};
  \node[data node] (7) [right of=6] {7};

  \node[call node] (9) [below of=0, ultra thick] {$r(0, -)$};
  \node[call node] (10) [below of=1] {$r(1, 0)$};
  \node[call node] (11) [below of=2] {$r(2, 1)$};
  \node[call node] (12) [below of=3] {$r(3, 2)$};
  \node[call node] (13) [below of=4] {$r(4, 3)$};
  \node[call node] (14) [below of=5] {$r(5, 4)$};
  \node[call node] (15) [below of=6] {$r(6, 5)$};
  \node[call node] (16) [below of=7] {$r(7, 6)$};

  \node[data node] (18) [below of=16] {7};
  \node[data node] (19) [below of=15] {6};
  \node[data node] (20) [below of=14] {5};
  \node[data node] (21) [below of=13] {4};
  \node[data node] (22) [below of=12] {3};
  \node[data node] (23) [below of=11] {2};
  \node[data node] (24) [below of=10] {1};
  \node[data node] (25) [below of=9] {0};

  %Input/Output next edges
  \path[thin]
    (0) edge node [above] {} (1)
    (1) edge node [above] {} (2)
    (2) edge node [above] {} (3)
    (3) edge node [above] {} (4)
    (4) edge node [above] {} (5)
    (5) edge node [above] {} (6)
    (6) edge node [above] {} (7)

    (18) edge node [above] {} (19)
    (19) edge node [above] {} (20)
    (20) edge node [above] {} (21)
    (21) edge node [above] {} (22)
    (22) edge node [above] {} (23)
    (23) edge node [above] {} (24)
    (24) edge node [above] {} (25);

  %Read/Write dependencies
  \path[dashed]
    (0) edge node [left] {} (9)
    (1) edge node [left] {} (10)
    (2) edge node [left] {} (11)
    (3) edge node [left] {} (12)
    (4) edge node [left] {} (13)
    (5) edge node [left] {} (14)
    (6) edge node [left] {} (15)
    (7) edge node [left] {} (16)

    (16) edge node [left] {} (18)
    (15) edge node [left] {} (19)
    (14) edge node [left] {} (20)
    (13) edge node [left] {} (21)
    (12) edge node [left] {} (22)
    (11) edge node [left] {} (23)
    (10) edge node [left] {} (24)
    (9) edge node [left] {} (25)

    (9) edge [bend right] node {} (10)
    (10) edge [bend right] node {} (11)
    (11) edge [bend right] node {} (12)
    (12) edge [bend right] node {} (13)
    (13) edge [bend right] node {} (14)
    (14) edge [bend right] node {} (15)
    (15) edge [bend right] node {} (16);

  %Control ependencies
  \path[ultra thick]
    (9) edge node [left] {} (10)
    (10) edge node [left] {} (11)
    (11) edge node [left] {} (12)
    (12) edge node [left] {} (13)
    (13) edge node [left] {} (14)
    (14) edge node [left] {} (15)
    (15) edge node [left] {} (16);

\end{tikzpicture}
\end{center}
\caption{DDG of a reverse operation}
\label{fig:reverse_ddg}
\end{figure}

If we now want to support efficient change propagation, we have to memoize each recursive call. Each call is not only dependent on the corresponding element of the input list, but also on the head of the allocator list, which will be the next successor of the newly created list element. Therefore, we have to memoize against the input element and the head of the accumulator list. 

When a value in the input list is updated, the corresponding call will be re-executed. Since this also re-allocates the memory that contains the new head of the accumulator list, no memo-match will ever occur. Memoization is useless for the entire program. Therefore, we also have to re-execute the next call, and so on. This causes a worst-case change propagation time of $O(n)$, even with constant sized updates. 

To circumvent this shortcoming, we can use the memory location of the next element as key for allocating the head of the accumulator list. Note that this does not result in allocating the memory location which is passed as key, but rather in allocating equal memory locations when two equal keys are passed. Since the resulting memory location for the head of the accumulator list will be the same as before change propagation, a memo match occurs and the next call is not re-executed by the change propagation algorithm. Using this approach, constant change propagation time is reached for constant sized updates in this example. 

This problem only occurs in a program that passes allocated memory as parameter to function calls that are memoized. The map implementation, for example, can benefit from change propagation without keyed allocation.  

\section{Trace Distance}
\label{sec:trace_distance}

Since each node of a DDG holds all information to uniquely identify a function call, the DDG can be used to compare two executions. For each pair of executions $(E, F)$, we count all the nodes that are in the DDG of $E$ but not in the DDG of $F$ and vice versa. The sum of those counts is called \textit{Trace Distance}. Trace distance provides an indicator about how much two executions of a program differ. During a change propagation, at least those differences have to be eliminated by re-executing the corresponding nodes. Therefore, the trace distance of two executions provides a lower bound for change propagation time between those executions, which is a useful feature. 

Calculating trace distance is simple: For two executions $(E, F)$, all nodes of $E$ that have an equivalent node in $F$ can be found using a greedy matching. The trace distance between the executions is the count of all other nodes. 

If we consult our simple map example with two executions that only differ in one input element, the trace distance will be exactly one, since only a single node will be re-evaluated. This is also illustrated in Figure \ref{fig:map_change_ddg_memo}.

It is important to realize that trace distance can be independent of memoization. The reason for this is that function calls that are not memoized can still be equal, due to having equal function arguments. Therefore, the memoized and unmemoized versions of map have equal trace distance for each pair of executions, even though change propagation times differ heavily.

\section{A more complex Example}
\label{sec:more_complex_example}

As already mentioned, the example from Section \ref{sec:simple_example} does not have dependencies between different function calls, and is therefore straight forward to dynamize. We therefore conclude this section with a more complex example. 

Many programming languages feature a function that is capable of combining all elements of a list with some associative operator, until there is only a single result left. This operation is known as \textit{reduce}, \textit{combine} or \textit{fold}.  

\subsection{Naive Approach}

Again, we consider a linked list $A = (a_1, ..., a_n)$ with list nodes $a_i = (v_i, n_i)$, each having a value and a next node, and a combining function $f(v_x, v_y) = r$, where $v_x$ and $v_y$ are input values and $r$ denotes the result. $v_x$, $v_y$ and $r$ are of the same type. 

A classic reduce operation would read the first two nodes $a_1$ and $a_2$ in the list, combine the value $v_1$ with the value $v_2$ and remember the result $f(v_1, v_2) = r_1$. Then, the next node $a_3$ is read, the value is combined with the previous result $r_1$ and the result $r_2$ is stored again. In general we can say that for each node $a_i$ we combine the value of the node $v_i$ with the previous result $r_{i-1}$. The last result $r_{n-1}$ is the final result of the function. 

If we now track dependencies for this operation, each result $r_i$ depends on the previous result $r_{i - 1}$ or the initial value $r_0$. Each result $r_i$ also depends on the value $v_i$ of the corresponding node $a_i$. In other words, the fold operation for each node $a_j, j > 1$ depends on the successors of the respective node $a_{j - k}, 0 < k < j$.

The dependency graph of such an implementation is shown in figure \ref{fig:naive_reduce}. 

%Foldl DDG
\begin{figure}
\begin{center}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2cm,
  thick,call node/.style={circle,fill=white!20,draw},
  data node/.style={rectangle,fill=white!20,draw}]

  \node[data node] (0) {0};
  \node[data node] (1) [right of=0] {1};
  \node[data node] (2) [right of=1] {2};
  \node[data node] (3) [right of=2] {3};
  \node[data node] (4) [right of=3] {4};
  \node[data node] (5) [right of=4] {5};
  \node[data node] (6) [right of=5] {6};
  \node[data node] (7) [right of=6] {7};

  \node[call node] (10) [below of=1, ultra thick] {f(0, 1)};
  \node[call node] (11) [below of=2] {f(1, 2)};
  \node[call node] (12) [below of=3] {f(3, 3)};
  \node[call node] (13) [below of=4] {f(6, 4)};
  \node[call node] (14) [below of=5] {f(10, 5)};
  \node[call node] (15) [below of=6] {f(15, 6)};
  \node[call node] (16) [below of=7] {f(21, 7)};

  \node[data node] (18) [below of=16] {28};

  %Input next edges
  \path[thin]
    (0) edge node [above] {} (1)
    (1) edge node [above] {} (2)
    (2) edge node [above] {} (3)
    (3) edge node [above] {} (4)
    (4) edge node [above] {} (5)
    (5) edge node [above] {} (6)
    (6) edge node [above] {} (7);

  %Read/Write dependencies
  \path[dashed]
    (0) edge node [left] {} (10)
    (1) edge node [left] {} (10)
    (2) edge node [left] {} (11)
    (3) edge node [left] {} (12)
    (4) edge node [left] {} (13)
    (5) edge node [left] {} (14)
    (6) edge node [left] {} (15)
    (7) edge node [left] {} (16)

    (16) edge node [left] {} (18)

    (10) edge [bend right] node {} (11)
    (11) edge [bend right] node {} (12)
    (12) edge [bend right] node {} (13)
    (13) edge [bend right] node {} (14)
    (14) edge [bend right] node {} (15)
    (15) edge [bend right] node {} (16);

  %Control ependencies
  \path[ultra thick]
    (10) edge node [left] {} (11)
    (11) edge node [left] {} (12)
    (12) edge node [left] {} (13)
    (13) edge node [left] {} (14)
    (14) edge node [left] {} (15)
    (15) edge node [left] {} (16);

\end{tikzpicture}
\end{center}
\caption{DDG of a naive reduce operation}
\label{fig:naive_reduce}
\end{figure}

If now a change to the first node in the input happens, a change propagation always has to re-execute the fold operation for the whole list, since every single intermediate result and the final result depend on the first node of the list. For this case, the asymptotic complexity of change propagation lies within $O(n)$, where $n$ denotes the input since. This expected bound also holds for the average case. The algorithm therefore updates in $O(n)$, which is not better than the expected time for a re-execution. 

Note that memoization is of no help here, since there are data dependencies from each function call to the result of the previous reduction. Every function call depends on all it's predecessors in the DDG.

\subsection{Tree-Like Implementation}

Combining all elements in a list can also be done in a \textit{tree-like} manner. This means that instead of combining all elements from the beginning to the end of the list, elements are pairwise combined, until the list is empty. 

More formally, for our input list $A_1 = (a_{1, 1}, a_{1, 2}, ..., a_{1, n})$, we combine each element with an odd index $a_i$ with its successor $a_{i + 1}$, and concatenate the results to a new list $A_2 = (a_{(2, 1)}, ..., a_{2, k})$. If the input list has an odd number of elements, the last element of the input list is added to the result list. Then, we repeat this operation with the result list, until there is only a single element left.

During each reduction step, half of the list is eliminated. That implies that for $n$ input elements, the count of reduction steps lies within the complexity class $O(log(n))$. If we consult the data dependencies, each element in the list $A_j, j > 1$ directly depends on two elements which are contained in the list $A_{j - i}$. Since there are only $O(log(n))$ reduction steps and therefore only $O(log(n))$ intermediate lists, the length of the path of dependencies from the input to each DDG node, including the final result, lies within $O(log(n))$. It is therefore possible to propagate the change of a constant number of values in $O(log(n)$. 

%Tree-Fold DDG
\begin{figure}
\begin{center}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2cm,
  thick,call node/.style={circle,fill=white!20,draw},
  data node/.style={rectangle,fill=white!20,draw}]

  \node[data node] (0) {0};
  \node[data node] (1) [right of=0] {1};
  \node[data node] (2) [right of=1] {2};
  \node[data node] (3) [right of=2] {3};
  \node[data node] (4) [right of=3] {4};
  \node[data node] (5) [right of=4] {5};
  \node[data node] (6) [right of=5] {6};
  \node[data node] (7) [right of=6] {7};

  \node[call node] (10) [below of=0, ultra thick] {f(0, 1)};
  \node[call node] (11) [below of=2] {f(2, 3)};
  \node[call node] (12) [below of=4] {f(4, 5)};
  \node[call node] (13) [below of=6] {f(6, 7)};

  \node[call node] (14) [below of=10] {f(1, 5)};
  \node[call node] (15) [below of=12] {f(9, 13)};

  \node[call node] (16) [below of=14] {f(6, 22)};

  \node[data node] (18) [below of=16] {28};

  %Input next edges
  \path[thin]
    (0) edge node [above] {} (1)
    (1) edge node [above] {} (2)
    (2) edge node [above] {} (3)
    (3) edge node [above] {} (4)
    (4) edge node [above] {} (5)
    (5) edge node [above] {} (6)
    (6) edge node [above] {} (7);

  %Read/Write dependencies
  \path[dashed]
    (0) edge node [left] {} (10)
    (1) edge node [left] {} (10)
    (2) edge node [left] {} (11)
    (3) edge node [left] {} (11)
    (4) edge node [left] {} (12)
    (5) edge node [left] {} (12)
    (6) edge node [left] {} (13)
    (7) edge node [left] {} (13)

    (10) edge node [bend left] {} (14)
    (11) edge node [left] {} (14)
    (12) edge node [left] {} (15)
    (13) edge node [left] {} (15)

    (14) edge node [bend left] {} (16)
    (15) edge node [left] {} (16)

    (16) edge node [left] {} (18);

  %Control ependencies
  \path[ultra thick]
    (10) edge node [left] {} (11)
    (11) edge node [left] {} (12)
    (12) edge node [left] {} (13)
    (10) edge node [left] {} (14)
    (14) edge node [left] {} (15)
    (14) edge node [left] {} (16);

\end{tikzpicture}
\end{center}
\caption{DDG of a tree-like reduce operation}
\label{fig:tree_reduce}
\end{figure}

Figure \ref{fig:tree_reduce} shows the DDG of the tree-like reduce operation. It can be recognized that there are no data dependencies that connect all nodes like before. The control dependencies, while still existent, do not cause any problems, since we can utilize memoization to stop change propagation between nodes that feature no data dependencies. 

While the upper bound of $O(log(n))$ holds for updates of values, this bound is not guaranteed for inserts or deletions of whole list elements from the input list. If we remove or add a single element, we change the index of all successive elements. Since we combine each odd element with its successor, changing the index of elements also changes which elements are combined. If now, for example, the first element is removed, we have to re-evaluate the reduction of all successors, which equals an re-evaluation for the entire list. 
It is possible to circumvent this drawback by using a slightly more complicated approach that involves randomization. Basically, elements are no longer combined pairwise, but based on pseudo-random numbers derived from the elements themselves. This way, the control flow is no longer dependent on the elements position in the input. While this approach is important for implementing a usable reduce operation, it is not further required in this writing. 
\chapter{The TBD Platform}
\label{ch:tbd_platform}

The principles of self-adjusting computation can be applied to functional programs. As shown by U. Acar et al in \cite{acar2007consistent}, this can be done because functional programs do not introduce cyclic dependencies into the program flow. Whether the concept of self-adjusting computation can also be transferred on imperative programs might have been answered by U. Acar et al in \cite{Acar2008}, where a platform for self-adjusting imperative programs was developed. This was done by providing an interface to necessary primitives like modifiables that supported an imperative program style. Open challenges, however, included providing a simplified interface and a platform that can be broadly used. For overcoming this challenges, \textit{TBD} was introduced. 

\textit{TBD}, short for \textit{To Be Determined}, is a platform for self-adjusting computation. By the time of this writing, TBD is in development\footnote{The source code and documentation of TBD can be found at \url{https://github.com/twmarshall/tbd/}} since early 2014 at Carnegie Mellon University by U. Acar and T. Marshal. TBD is implemented in \textit{Scala}, an object-functional language executable on top of the \textit{Java Virtual Machine}. All programs written on top of TBD are self-adjusting and therefore incremental.

For calculating trace distance, the implementation of the underlying platform is of importance, since it influences how programs are structured and how DDG nodes represent function calls. Therefore, this section discusses how TBD provides self-adjusting computation, how variables are represented, and especially how programs on top of TBD are written. In the next chapter, we discuss a specific DDG format for TBD in detail. 

\section{Programming Interface}

The usage of a special memory model enables TBD to track read-write dependencies. When such a dependency is detected, the code that depends on the changed variable has to be identified and re-evaluated. This requires that the environment of the previous execution is still available. For many programming languages, this task is not straight-forward. Scala, however, supports the usage of closures. \textit{Closures} are functions that are paired with their enclosing environment. This means that a closure can hold all information necessary to repeat a function evaluation. Furthermore, closures can be treated and passed around like variables. TBD adds a closure to execute to each read, memo or allocation call as parameter. The corresponding function calls the closure during execution. 

It is important to note that all functions of programs written on top of TBD have to be side-effect free and deterministic. This means that no function is allowed to write global variables. Also, two function calls with the same arguments must produce the same outcome. If this is not respected by a program, change propagation will fail, since the corresponding dependencies can not be tracked. 

TBD provides a broad set of API functions. Not all parts are important for this writing and therefore left out for clarity. The relevant parts are summarized in \ref{fig:tbd_uml}. The classes $Function0$ and $Function1$ are included from the Scala API and represent closures. $Function0$ only has a return value of type $R$. $Function1$ has a return value of type $R$ and a single parameter of type $T1$. The static class $TBD$ provides important methods for manipulating memory. Since those methods are semantically treated like control structures and can be called without specifying the name of the $TBD$ class, they are refereed to as functions of TBD. It should be noted that the type parameters $U$ and $T$ are not type parameters of the $TBD$ class but of the functions provided by the $TBD$ class. Therefore, those type parameters are bound at call level and can be different for distinct calls. Important functions of TBD will be described in depth during this section. Also, we will discuss how certain calls behave during change propagation, where relevant. 

\begin{figure}
\begin{center}
\includegraphics[scale=0.7]{uml/TBD.pdf}
\end{center}
\caption{An UML class diagram for parts of the TBD API}
\label{fig:tbd_uml}
\end{figure}


\subsection{Memory Model}

For accomplishing self-adjusting computation, it is necessary to detect when a memory location is read or written. With a classic machine model featuring random access memory, reads and writes can be hard to track. Furthermore, high-level languages like Scala usually do not provide primitive data types to reference memory locations. Therefore, all variables that need to be tracked are wrapped into so called \textit{modifiables} or short \textit{mods}, which are typed references to some storage location. Each mod has a unique identifier that can be used to reference the modifiable through the program. All mods have to be explicitly allocated, read and written by calling corresponding functions. It is therefore possible to recognize dependencies between allocations, writes and reads of a mod.

A mod is represented by an instance of the class $Mod[T]$, where $T$ specifies the type of the object stored into the modifiable. Besides the $id$, a mod has no public functions or parameters. A mod can, however, be manipulated by certain functions of TBD.

\subsection{Allocation and Writing of Modifiables}
\label{sec:allocation}

Mods are allocated by calling the $mod[T]$ function. $T$ denotes the type of the created mod. The $mod$ function requires a closure without any parameters as first argument. The code contained in this closure is responsible for writing to the allocated memory location exactly once, by issuing a single $write[T]$ call of the same type $T$. 

The write call has a single parameter of type $T$, the value that has to be stored in the mod.
The write call must be the last operation executed within the closure. This is enforced by requiring a return type of $Changeable[T]$ from the closure. The class $Changeable[T]$ can only be instantiated by the $write[T]$ function. Since Scala automatically returns the result of the last operation in a function call as result for that function, this type constraint is satisfied by placing the $write[T]$ at the end of the function. Of course there are ways to circumvent this constraint, for example by issuing a write call and remembering the resulting $Changeable[T]$ for later return, with other operations after the write call. While this is possible and also might produce a working program, such an approach will break self-adjusting computation in most cases and should therefore be avoided.

When the $mod$ function is called, memory for a mod is allocated. Then, the closure is invoked by the $mod$ function. As last operation inside the closure, a value is written into the allocated mod using the $write$ operation. As soon as the closure finished execution, an instance of $Mod[T]$ is returned by the $mod$ function. The returned mod contains the value that has been written inside the closure. 

\subsubsection{Keyed Allocation of Modifiables}
\label{sec:keyed_alloc_tbd}

 Keyed allocation is provided by an overloaded version of the $mod[T]$ function, which accepts a key of any type as second parameter. Aside from allocating different memory dependent on a key, the overload behaves exactly like the normal $mod$ function. Allocating a mod with a key that has been used before to allocate another modifiable will result in two modifiables with equal ids, which will use the same memory location internally. 

\subsubsection{Context-Sensitive Keyed Allocation of Modifiables}

As mentioned, any object can be used as a key for the allocation. In the reverse example from \ref{sec:keyed_alloc}, we used each input element of the list as key for the modifiable that holds the corresponding output element. However, there might exist more than one function in a program that follows this approach, especially, when frameworks are used. In a case where the same input list is passed to two such function calls, one of the functions would re-write memory locations the other function has allocated. To avoid this situation, the $Modizer$ class was introduced. Instances of the $Modizer$ class provide a public $apply$\footnote{Due to a feature of the Scala language, it is possible to call the $apply$ method on an instance without specifying the method name. That means that for a $Modizer$ instance with name $modizer$, the $apply$ method can be called by using $modizer.apply(...)$ or $modizer(...)$ with the same outcome.} method, with the same signature and functionality as the keyed $mod$ function. However, the key comparison only takes place for all calls to the $apply$ method of the same instance. For two different instances of $Modizer$, identical calls to the $apply$ method would still lead to the creation of two different mods. By passing a $Modizer$ instance to a function call, it is therefore possible to isolate the process of keyed allocation from other parts of the program. An instance of $Modizer$ can be created by calling the parameterless $makeModizer$ function of TBD. 

\subsection{Reading of Modifiables}

To read a mod, the $read[U]$ function has to be used. The $read[U]$ function requires an instance of $Mod[U]$ as first parameter, and a closure as second parameter. The closure has to take a single parameter of type $U$, the value of the mod that has been read. The return type of the closure has to be $Changeable[T]$, and the $read[U]$ call also returns an instance of $Changeable[T]$. A call to $write[T]$ must be the last call within the passed closure and the $read[U]$ call has to be placed inside the closure of a $mod[T]$ call. In other words, it is possible to nest any numbers of reads between an allocation and the corresponding write. 
 
When the $read[T]$ function is called, it invokes the closure and passes the value of the mod that has been read as parameter to the closure. The passed value is only valid within that closure and must not be assigned to any variable outside the closure. 

\subsubsection{Updating Reads during Change Propagation}

If the value of a modifiable is updated from outside the program, the change propagation algorithm finds alls $read$ calls that read the changed modifiable. Then, the closure passed to the $read$ call is invoked with the new value of the modifiable as parameter. The $read$ call itself is not re-executed. If multiple modifiables are updated, the $read$ calls are updated in topological order. Also, modifiables that are re-written during change propagation might be read somewhere else in the program, causing further needs for updates. Therefore, change propagation can cause cascading updates.

In other words, $read$ calls are used to identify parts of the program that have to be updated during change propagation. 

\subsection{Memoization}

TBD uses explicit memoization. This is done using a $memo$ operation. The first argument of the $memo$ operation is a list of objects that represent the state of the function to call.The second parameter is the closure to memoize. In the general case, the argument list contains the arguments that are bound to the closure. 

When the $memo$ operation is called, and has not been called with the same argument list before, the closure is executed and the result of the closure is returned by the $memo$ call. Additionally, the result of the closure for the given argument list is stored. Since a memo operation can only remember a single pair of arguments and result, this operation overwrites any previously stored result for any argument list. If the $memo$ operation is now called again with the same argument list, the stored result is returned without re-executing the closure. This occurrence is called \textit{memo match}. It is important to remember that the closure itself is not a part of the argument list. 

\subsubsection{Context-Sensitive Memoization}

The problem outlined in section \ref{sec:keyed_alloc_tbd} also exists with memoization: Two distinct functions might memoize against the same arguments. In this case, memo matches would occur despite the closure is a different one. To solve this problem, an instance of the $Memoizer$ class can be passed to each function call. $Memoizer$ provides an $apply$ method with the same signature and functionality as the $memo$ function. Memo-matches are only checked for calls to the same instance. That means that for two different instances of the $Memoizer$ function, identical calls can return different results. 

\subsubsection{Memoization and Change Propagation}
Memo matches are designed for and happen during change propagation. If a $memo$ call is re-executed and a memo-match occurs, re-execution is halted at that point, since the closure of the $memo$ call will not be invoked. It is important to realize that without correct $memo$ calls, each update potentially re-executes the whole program: Updating the first read of the program would re-execute the corresponding clause that recursively calls the whole program again. 

In short, $memo$ calls are used to stop a running change propagation from re-evaluating program parts where re-execution is not necessary. 

\section{Writing programs for TBD}

Since writing TBD programs can be complicated due to all introduced constraints, this section explains how basic TBD programs are implemented and what happens during execution. Also, Scala features necessary for implementing TBD programs are explained briefly. 

As mentioned before, TBD enforces that the last call in each closure passed to $read$ or $mod$ to be a $write$ call. This means that in TBD programs, calls are nested, with a $mod$ call at the top, one or more $read$ calls beneath, and a $write$ call at the bottom. Since a program might use more complex data structures, new modifiables may also be created inside the $read$ calls. Also, the closure-driven programming model does not support loops. Therefore, TBD programs that work on sets of input values are recursive. 

\subsection{TBD Context} 

It should be noted that each of the calls described in the previous section requires another argument of the type $Context$, which holds information internally necessary for the operation of TBD. This argument is, however, marked as \textit{implicit} and declared in a secondary parameter list. This means that the Scala compiler automatically passes an available instance of $Context$ to the function. This way, we can simply call the function without specifying the parameter $Context$. If own functions are introduced, however, the programmer has to be careful to add an implicit parameter for $Context$ to the function, so an instance is available inside the function body. The reason a second parameter list is used is that this exploits Scala mechanics to allow the programmer to write cleaner code \footnote{Type inference is done by parameter list in Scala. Using a separate parameter list for the implicit $Context$ parameter enables the compiler to infer all parameter types. Also, if the only parameter of a parameter list is a closure, this closure can be written as code block instead of a full function declaration. This way, $mod$, $read$ and $memo$ get the semantics of control structures.}.

A function declaration featuring a secondary parameter list with an implicit parameter of type $Context$ can be seen in \ref{code:second_param}. The functions name is $function$, the primary parameter list contains a single parameter of type $Mod[Int]$ with the name $parameter$. The return type is not specified and will be inferred by the compiler. 

\begin{figure}
\begin{lstlisting}[frame=single,basicstyle=\ttfamily,numbers=left]
def function(parameter: Mod[Int])(implicit context: Context)
\end{lstlisting}
\caption{Declaration of a second parameter list}
\label{code:second_param}
\end{figure}

\subsection{Add: A basic Example}
\label{sec:tbd_basic_example}
As a basic example, we discuss a function that takes two numbers, adds them and returns the result. To implement this functionality on top of TBD, we have to create a function that takes two mods as parameters. First, the function has to memoize against the input, using a $memo$ call. Then, the function has to allocate a mod for the result by issuing a $mod$ call. In the closure passed to the $mod$ call, the input variables have to be read. This is done by issuing two $read$ statements that are nested. In the closure passed to the innermost $read$ call, both input values are available to the program. Those values have to be added and written, using a $write$ call. 

\begin{figure}
\begin{lstlisting}[frame=single,basicstyle=\ttfamily,numbers=left]
def add
  (aMod: Mod[Int], bMod: Mod[Int])
  (implicit context: Context): Mod[Int] = {
  memo(aMod, bMod) {
    mod {
      read(aMod) {
        case a => read(bMod) {
          case b => write(a + b)
        }
      }
    }
  }
}
\end{lstlisting}
\caption{A basic example of a function that adds two mods, utilizing $read$, $write$, and $mod$}
\label{code:add_example}
\end{figure}

Figure \ref{code:add_example} shows an implementation of this example. The first line is a function declaration in Scala. The function's name is $add$. The parameter list is located in the second line. The function accepts two parameters $aMod$ and $bMod$, both of type $Mod[Int]$ and returns a $Mod[Int]$. $Mod[Int]$ is a modifiable holding an integer. In the third line, an additional parameter list is shown, containing the implicit $Context$ parameter. This is done because the calls to $read$, $write$, $memo$ and $mod$ need an instance of $Context$ in the scope of their call.

The fourth line is responsible for memoization. In this case we check for memo matches using the input variables, $aMod$ and $bMod$. Note that we are able to pass a code block to the function. This code block is automatically converted to a closure by the Scala compiler. Whenever no memo match occurs, the $memo$ operation will call this code block. We memoize against the input mods, because for equal pairs of input mods, the output will be the same and we do not need to propagate changes trough this function. If the value of the input mod was updated, the change propagation algorithm will take care of updating the reads, where appropriate. 

The fifth line allocates a new modifiable using the $mod$ function, without any key. 

In line six, we read the first argument, $aMod$. 

In the seventh line, the read value is bound to the local variable $a$ using a $case$ statement from the Scala pattern matching engine. This is merely a trick so we can omit declaring full functions and just place our code into a code block instead. The statement $case$ $a$ $=>$ indicates the default case, any value of any type is accepted and bound to $a$. This can be noticed because there is no type constraint and also no condition associated with this case statement. Also, we issue a read call that reads the second parameter, $bMod$.

In the eight line, the read value of the second parameter is bound to the local variable $b$. Then, the sum $a + b$ is calculated and written to the allocated mod using a $write$ call. 

When executing this program, each function executes the corresponding closure before returning. This means that all calls are executed in the order described above and also that the $mod$ call returns the result not until the $write$ operation has finished. 

If an input value of this function changes, TBD will take care to re-evaluate the corresponding $read$ call. The $read$ call then invokes its respective closure again and all function calls in the closure will be executed. Finally, the write will be re-executed and the result will be updated. If the $add$ function is called by another function during change propagation, the $memo$ call checks for memo matches and halts change propagation if the function is called with the same parameters as before. 

\subsection{Map: A more complex Example}
\label{sec:tbd_memo_map}

To wrap up this section, we revisit the map example introduced in section \ref{sec:simple_example} and theoretically discussed in section \ref{sec:ddg}. Since the map operator operates on lists, we define a list element class, $SimpleList$, that is shown in \ref{fig:simple_list_uml}. The $SimpleList$ class represents a single list element. Each instance has a member that points to the next element of the list, and a member that holds the value. Like the member $next$, the head of a list is represented by an instance of $Mod[SimpleListElement[T]]$. The last element in the list is represented by a modifiable that holds a null reference.  

\begin{figure}
\begin{center}
\includegraphics[scale=0.7]{uml/SimpleList.pdf}
\end{center}
\caption{An UML class diagram for a simple linked list}
\label{fig:simple_list_uml}
\end{figure}

Since the list head and each next element are represented as modifiables, we introduce a wrapper class that adds a map method to instances of $Mod[SimpleListElement[T]]$, and mark it as implicit. This way, the Scala compiler automatically wraps all instances of $Mod[SimpleListElement[T]]$ where applicable. 

\begin{figure}
\begin{lstlisting}[frame=single,basicstyle=\ttfamily,numbers=left,mathescape=true]
def map[U]
  (mapper: T => U, 
  memoizer: Memoizer[Mod[SimpleList[U]]])
  (implicit c: Context):
    Mod[SimpleList[U]] = {
  memoizer(currentElem) {
    mod {
      read(currentElem) {
        case null => write(null)
        case elem => {
          val newValue = mapper(elem.value)
          val newNext = elem.next.map(mapper, memo)
          write(new SimpleList(newValue, newNext))
        }
      }
    }
  }
}
\end{lstlisting}
\caption{A map function implemented on top of TBD}
\label{code:map_example}
\end{figure}

In our wrapper class, we define the method $map$. The implementation of $map$ can be seen in Listing \ref{code:map_example}. As arguments we supply a function parameter, $mapper$, that maps the value of each element of type $T$ to a new value of type $U$, and an instance of the $Memoizer$ class. 

In line six we memoize against the $Mod[SimpleList[T]]$ instance $currentElement$, which represents the list element we are operating on. We do this because for equal list elements, the result will be equal. Then, we create a new modifiable for the new element in line seven and read the current element in line eight. 

We now use the Scala pattern matching engine to handle two different cases. The statement $case$ $null$ $=>$ indicates the case where the value passed to the closure is $null$. The statement $case$ $elem$ $=>$ indicates the default case, like before. 

If the current element is null, we reached the end of the list, and we write the final element of our output list in line nine. If the current element is not null, we write a new list element in line thirteen. The value of the new list element is the result of the call to the $mapper$ function with the value of the input element as value in line eleven. The next element is the result of a recursive application of the $map$ function to the successor of this element in line twelve. 

If an input value is updated, the corresponding $read$ call is re-evaluated, and therefore the closure starting at line eight gets re-executed. This also re-executes the call to $mapper$ and the recursive call to $map$ for the next element. If the next element was not modified, a memo match occurs and change propagation halts for this specific branch and the recursive $map$ call returns. Then, the $write$ call is re-executed with the updated result. 

\chapter{DDGs for TBD}
\label{sec:tbd_ddgs}

DDGs are fundamental for calculating trace distance. However, TBD does not perform change propagation on a level of program functions, but rather by re-evaluating closures that are passed to $read$, $write$, $memo$ and $mod$. Also, TBD uses a DDG internally to handle change propagation, but this DDG does not hold enough properties to calculate trace distance. Therefore, we introduce a new, more specific DDG format for TBD. The new DDG format introduces nodes for $read$, $mod$, $memo$ and $write$ operations. 

Each node in the DDG has to hold enough information to re-evaluate the corresponding call. Since we have to compare DDG nodes for calculating trace distance, all data has to be available in comparable form, including closures. We therefore introduce the following representations: 

\begin{itemize}

\item A \textit{Function Tag} is a data structure that represents a closure in a compact way. A function tag consists of a unique function id that identifies the closure and a list of free variables inside the function that are bound from an outer scope, including the name and the value of the variable. This detail is important, since those variables can influence the closure behavior.

\item Each node has an unique node id and a node type, either $read$, $write$, $memo$, $mod$ or $root$. Whenever a new node is created, for example during re-evaluation, a new id is generated for the new node.

\item A read node contains the modifiable and value that has been read and a function tag that represents the closure that was called by the $read$ function. 

\item A write node contains the value written and the id of the mod written to. 

\item A memo node contains the list of arguments used to check for memo matches and a function tag that identifies the closure that was called by the $memo$ function. 

\item A mod node contains the mod created and a function tag representing the closure called by the $mod$ function.

\item A root node has no properties. However, a root node might have multiple child nodes. 

\end{itemize}

Those properties, except the id, completely define the state of the called function. With the information available for a node, we can re-execute the corresponding call. Also, for two nodes with equal properties, the corresponding call will execute equally. Child nodes, and therefore sub calls, might execute differently, since values read by $read$ calls might have been updated. 

The edges of the DDG represent control dependencies. Therefore, the DDG resembles the call tree of a program execution, with the root node as root of the tree. Other dependencies, like data dependencies or allocate-write dependencies, are not explicitly given, but can be inferred from the node properties. Since each node holds the state of the corresponding function call, the DDG holds all information necessary to reconstruct the whole execution of the program. 

\section{Visualization of DDGs}

Since the DDGs of TBD programs are important through this writing, we define a visualization format for those DDGs. An example can be seen in figure \ref{fig:add_ddg}. This specific DDG will be discussed later in detail. The type of each node is explicitly annotated and prepended with the node id in the figure. 

An excerpt of the node properties of the function call is annotated below the node id and type. Note that this annotation does not contain all properties of the node. Also, this annotation, while very similar, does not correspond to the signature of the function call. 

All variables in the annotation that are of the form $d.\alpha$ refer to mods, where $\alpha$ refers to the id of the modifiable. All variables of the form $f.\beta$ refer to closures in the source code, where for two variables with equal $\beta$, the same function has been called, although the arguments might have been different. We use numeric names because our closures are anonymous and do not have names. 

The specific annotations for each node type are the following:
\begin{itemize}
\item Memo nodes have any number of values in their annotation. The last value corresponds to the closure called by the $memo$ function. All other values correspond to the argument list used to check for memo matches. 

\item Mod nodes have two values in their annotation. The first one corresponds to the allocated mod, the second one to the closure being called. 

\item Read nodes have three values in their annotation. The first one corresponds to the mod being read, the second one to the value of the read mod, and the third one to the closure being called.

\item Write nodes have two values in their annotation. The first one is the value being written, the second one the mod the value is written to. 
\end{itemize}

While those annotations do not completely identify each node, they are sufficient for understanding what node corresponds to which part of the program. Further details will be pointed out where applicable.   

Also, the input and output of the program are not represented in this format, data dependencies are also not explicitly marked. These details are intentionally omitted, since for complex functions, those graphs would get unclear. The control dependencies, however, can clearly be seen. Control edges always go from top to bottom. 

\section{Example: DDG of Add}

Figure \ref{fig:add_ddg} shows the DDG of a call to the $add$ function from listing \ref{code:add_example}, with the input values $1$ and $2$.

\begin{figure}
\centering
\begin{tikzpicture}[font=\sffamily,very thick,level/.style={sibling distance=30mm, level distance=10mm}]
\tikzstyle{every node}=[font=\small]
\node [root, label={0:1.root}]{}
child {
  node [memo, label={0:2.memo}, label={350:(d.1, d.2, f.0)}]{}
  child {
    node [mod, label={0:3.mod}, label={350:(d.3, f.1)}]{}
    child {
      node [read, label={0:4.read}, label={350:(d.1, 1, f.2)}]{}
      child {
        node [read, label={0:5.read}, label={350:(d.2, 2, f.3)}]{}
        child {
          node [write, label={0:6.write}, label={350:(3, d.3)}]{}
        }
      }
    }
  }
};
\end{tikzpicture}
\caption{DDG of the simple $add$ function}
\label{fig:add_ddg}
\end{figure}

In our example, it can be seen that the $memo$ call memoized two modifiables, with the ids $d.1$ and $d.2$. Those modifiables are our input mods. Then, the $memo$ call invoked an anonymous function $f.0$. The function $f.0$ refers to the function starting at line four in our example, in which a $mod$ call was executed. The $mod$ call then allocated a new modifiable with id $d.3$ and called the anonymous function $f.1$, starting in line five of our example. Inside this function, $d.1$ with value $1$ and $d.2$ with value $2$ were read by two consecutive $read$ calls. Finally, the innermost anonymous function, $f.3$, that refers to the function starting in line seven, is executed. Inside this function call, the result $3$ is written to the allocated modifiable $d.3$. 
  
When observing the ids of the modifiables in $read$, $mod$ and $write$ executions closely, dependencies can be found. In this case, we have an allocate-write dependency between the $mod$ and the $write$ instruction, since we allocate the mod $d.3$ and then write to it. 

\section{Example: DDG of Map}

%generated using: 
%bin/visualization.sh -a smmap -t manual -i 1  
%u 1 0
%a 2 1
%a 3 2
%a 4 3
%p

\begin{figure}
\centering
\begin{tikzpicture}[font=\sffamily,very thick,level/.style={sibling distance=40mm, level distance=10mm}]
\tikzstyle{every node}=[font=\small]
\node [root, label={0:1.root}, label={350:}]{}
child {
  node (2)[memo, label={0:2.memo}, label={350:(1, d.0, f.23)}]{}
  child {
    node (3)[mod, label={0:3.mod}, label={350:(d.2, f.22)}]{}
    child {
      node (4)[read, label={0:4.read (d.0, 0, f.21)}]{}
      child {
        node (10)[memo, label={0:10.memo}, label={350:(1, d.1, f.23)}]{}
        child {
          node (11)[mod, label={0:11.mod}, label={350:(d.7, f.22)}]{}
          child {
            node (12)[read, label={0:12.read (d.1, 1, f.21)}]{}
            child {
              node (13)[memo, label={0:13.memo}, label={350:(1, d.4, f.23}]{}
              child {
                node (14)[mod, label={0:14.mod}, label={350:(d.8, f.22)}]{}
                child {
                  node (15)[read, label={0:15.read (d.4, 2, f.21)}]{}
                  child {
                    node (16)[memo, label={0:16.memo}, label={350:(1, d.5, f.23)}]{}
                    child {
                      node (17)[mod, label={0:17.mod}, label={350:(d.9, f.22)}]{}
                      child {
                        node (18)[read, label={0:18.read (d.5, 3, f.21)}]{}
                        child {
                          node (19)[memo, label={0:19.memo}, label={350:(1, d.6, f.23)}]{}
                          child {
                            node (20)[mod, label={0:20.mod}, label={350:(d.10, f.22)}]{}
                            child {
                              node (21)[read, label={0:21.read}, label={350:(d.6, null, f.21)}]{}
                              child {
                                node (22)[write, label={0:22.write}, label={350:(d.10, null)}]{}
                              }
                            }
                          }
                        }
                        child {
                          node (23)[write, label={0:23.write}, label={350:(d.9, 6)}]{}
                        }
                      }
                    }
                  }
                  child {
                    node (24)[write, label={0:24.write}, label={350:(d.8, 4)}]{}
                  }
                }
              }
            }
            child {
              node (25)[write, label={0:25.write}, label={350:(d.7, 2)}]{}
            }
          }
        }
      }
      child {
        node (26)[write, label={0:26.write}, label={350:(d.2, 0)}]{}
      }
    }
  }
};
\begin{pgfonlayer}{background}
\fill[blue,opacity=0.2] \convexpath{2,26,4,3}{8pt};
\fill[blue,opacity=0.2] \convexpath{10,25,12,11}{8pt};
\fill[blue,opacity=0.2] \convexpath{13,24,15,14}{8pt};
\fill[blue,opacity=0.2] \convexpath{16,23,18,17}{8pt};
\fill[blue,opacity=0.2] \convexpath{19,22,21,20}{8pt};
\end{pgfonlayer}
\end{tikzpicture}
\caption{DDG of the $map$ function with input $(0, 1, 2, 3)$.}
\label{fig:map_tbd_ddg}
\end{figure}


Figure \ref{fig:map_tbd_ddg} shows the DDG of a call of the map function with the input list $(0, 1, 2, 3)$ and a mapping function that multiplies each element by two. Regarding the basic structure, the DDG is similar to the DDG in figure \ref{fig:map_ddg}. The node sets $\{2, 3, 4, 26\}$, $\{10, 11, 12, 25\}$, $\{13, 14, 15, 24\}$ and $\{15, 17, 18, 23\}$ correspond to a single call of the map function from figure \ref{fig:map_ddg} each. The nodes $\{19, 20, 21, 22\}$ represent writing the end of the list, which can be recognized by reading a $null$ element and then writing $null$. 

Each of those sets corresponds to a single recursive call of the $map$ function described in listing \ref{code:map_example}. Those sets can be identified by consulting the function ids inside the signatures. For instance, the memo node at the beginning of each call invokes the closure $f.23$, corresponding to the closure passed to the memo call in listing \ref{code:map_example}. All sets those are highlighted in figure \ref{fig:map_tbd_ddg}.

For each recursive call, the corresponding part of the DDG is similar. We can see that a memo call occurred, with a mod and the number $1$ in the argument list. The number $1$ originates from the memoizer and is internally used to match a stored memoization entry with its memoizer. The mod in the argument list corresponds to the input element that is currently being mapped. After the memo call, a new mod was allocated. Then, the input element was read and the same function was called recursively for the next element. After the recursion finished, the result was written to the previously allocated mod. We can recognize this dependency because the mod ids of the allocation and the write call match. 

Figure \ref{fig:map_tbd_ddg_change} shows the DDG after the second value in the input was updated to four. This is the same operation as conducted in the example in \ref{sec:ddg}, an update of the second element of the input list. During change propagation, TBD re-evaluated the closure passed to the read node $12$, that read the updated value. Inside the closure, the mapping function was re-executed. The nodes $13$ and $27$ are direct descendants of node $12$. Node $13$ is a memo node that memoizes against the successor of the changed element. Since the successor was not modified, a memo match occurred and change propagation stopped for this branch. Node $27$ is a write call that got re-executed. The new result, eight, was written to the modifiable. The two nodes that were involved in the update are highlighted in figure \ref{fig:map_tbd_ddg_change}. It is important to realize that node $12$ was not replaced in the DDG, since only the corresponding closure was re-executed. Also, no modifiables were newly allocated. The only change was the update of a single output value. 

%generated using: 
%bin/visualization.sh -a smmap -t manual -i 1  
%u 1 0
%a 2 1
%a 3 2
%a 4 3
%p
%u 1 4

\begin{figure}
\centering
\begin{tikzpicture}[font=\sffamily,very thick,level/.style={sibling distance=40mm, level distance=10mm}]
\tikzstyle{every node}=[font=\small]
\node (1)[root, label={0:1.root}, label={350:}]{}
child {
  node (2)[memo, label={0:2.memo}, label={350:(1, d.0, f.23)}]{}
  child {
    node (3)[mod, label={0:3.mod}, label={350:(d.2, f.22)}]{}
    child {
      node (4)[read, label={0:4.read (d.0, 0, f.21)}]{}
      child {
        node (10)[memo, label={0:10.memo}, label={350:(1, d.1, f.23)}]{}
        child {
          node (11)[mod, label={0:11.mod}, label={350:(d.7, f.22)}]{}
          child {
            node (12)[read, label={0:12.read (d.1, 4, f.21)}]{}
            child {
              node (13)[memo, label={0:13.memo}, label={350:(1, d.4, f.23)}]{}
              child {
                node (14)[mod, label={0:14.mod}, label={350:(d.8, f.22)}]{}
                child {
                  node (15)[read, label={0:15.read (d.4, 2, f.21)}]{}
                  child {
                    node (16)[memo, label={0:16.memo}, label={350:(1, d.5, f.23)}]{}
                    child {
                      node (17)[mod, label={0:17.mod}, label={350:(d.9, f.22)}]{}
                      child {
                        node (18)[read, label={0:18.read (d.5, 3, f.21)}]{}
                        child {
                          node (19)[memo, label={0:19.memo}, label={350:(1, d.6, f.23)}]{}
                          child {
                            node (20)[mod, label={0:20.mod}, label={350:(d.10, f.22)}]{}
                            child {
                              node (21)[read, label={0:21.read}, label={350:(d.6, null, f.21)}]{}
                              child {
                                node (22)[write, label={0:22.write}, label={350:(d.10, null)}]{}
                              }
                            }
                          }
                        }
                        child {
                          node (23)[write, label={0:23.write}, label={350:(d.9, 6)}]{}
                        }
                      }
                    }
                  }
                  child {
                    node (24)[write, label={0:24.write}, label={350:(d.8, 4)}]{}
                  }
                }
              }
            }
            child {
              node (27)[write, label={0:27.write}, label={350:(d.7, 8)}]{}
            }
          }
        }
      }
      child {
        node (26)[write, label={0:26.write}, label={350:(d.2, 0)}]{}
      }
    }
  }
};
\begin{pgfonlayer}{background}
\fill[red,opacity=0.5] \convexpath{12, 27}{8pt};
\end{pgfonlayer}
\end{tikzpicture}
\caption{DDG of the $map$ function after the second input value was updated}
\label{fig:map_tbd_ddg_change}
\end{figure}


\chapter{Calculating Trace Distance}
\label{ch:calc_trace_distance}

Consulting DDGs can greatly ease the debugging of TBD programs. Not only bugs in algorithms can be found quicker, also the performance of the program can be analyzed. When we update the input and run change propagation for any program, the program state updates and therefore the DDG changes. By comparing the DDGs of two executions of the same program with two different inputs each, we can inspect how well the program handled change propagation. As already mentioned in \ref{sec:trace_distance}, this difference is called \textit{Trace Distance}. 

The trace distance of two DDGs basically indicates the number of closures to re-execute during change propagation. A constant increase in trace distance indicates a constant increase in execution time. It is feasible to assume that each closure, excluding sub calls to $mod$, $read$ or $memo$ the closure contains, can be executed in constant time. The reason is that control structures like loops are not available in TBD programs. Iterating over input elements has to be done using recursion, where each recursion step places new $read$ calls. The $read$ call will correspond to a new closure that will be counted separately if applicable. It is possible to write programs that do not satisfy this condition. An example would be a program that stores the whole input into a single mod, reads that mod and iterates over the whole input at once. Such a program would, however, perform bad when propagating input changes and miss the point of incremental computation.

As already mentioned, calculating trace distance is straight forward using a greedy matching. We create two sets $A$ and $B$, where $A$ holds all nodes of the first DDG and $B$ holds all nodes of the second DDG. For each node in $A$, we search $B$ for an equivalent node. If we find an equivalent node, we remove both nodes from $A$ and $B$, respectively. The count of remaining nodes, or formally $|A| + |B|$ is the trace distance of the two DDGs. 

The interesting part here, however, is how we define equality of nodes. In this chapter, we explain how it is possible to create multiple, meaningful definitions of equality that result in different meaningful results for trace distance. 

Especially regarding function tags, it has to be noted that comparing two functions for equality is unsolved in general. This problem is also known as \textit{Entscheidungsproblem} as described by A. Church in \cite{church1936note}. However, we can exploit that TBD functions have to be side-effect free, which means that for the same parameters, we will get the same results. Child $read$ calls form an exception, but those are handled by change propagation. 
  
\section{Types of Trace Distance}
For checking node equality, we introduce so called \textit{node tags}. A node tag is a ordered set of values associated with a single node. When checking nodes for equality, we simply check whether all values in the node tag of both nodes are equal. In the following sections, we will introduce multiple forms of node tags that correspond to different metrics. Node tags usually consist of subsets of the properties introduced while explaining TBD DDGs in section \ref{sec:tbd_ddgs}.

Since tags can also contain values that are read or written in the program, it is important that all data structures used by the program implement a meaningful method to check for deep equality. Modifiables form an exception and can are compared by comparing the id of the modifiable, regardless of the stored value. 

For each type of trace distance, proofs of correctness can be found in appendix \ref{sec:correctness}.

\subsection{Pure Trace Distance}

The most basic form of trace distance is induced by comparing nodes by their unique ids: It is sufficient to include only the unique id in the node tags for comparison. This means that if a node is removed or added, it will increase the trace distance by one. If a node is re-executed, it will count twice. We call this form of trace distance \textit{pure trace distance}. For two DDGs $A$ and $B$ of two successive executions, we denote pure trace distance as $\delta_p(A, B)$.

Since pure trace distance basically counts all nodes that were removed or added from the DDG while transforming the program into an updated state, it reflects how efficient change propagation was done in practice. Therefore, pure trace distance is only meaningful for two successive pairs of executions. Comparing node ids for non-successive pairs of executions is not meaningful, since two identical program runs might lead to nodes with differing ids, depending on the executions done before. 

Pure trace distance is useful for comparing two versions of a program and to investigate which one performed better under certain loads.

\subsection{Memo-Sensitive Trace Distance}

An interesting aspect of a self-adjusting program is how well memoization was handled. For this purpose, we introduce \textit{memo-sensitive trace distance}. For two DDGs, $A$ and $B$, we denote memo-sensitive trace distance as $\delta_m(A, B)$.

To implement memo-sensitive trace distance, we include all properties introduced in section \ref{sec:tbd_ddgs} except the node id into the tag. This is done because, as mentioned before, the properties of each node completely define the state of the corresponding call. If all properties of two nodes equal, the corresponding functions will execute equally. Those nodes are not counted by memo-sensitive trace distance, since their re-evaluation can be omitted using memoization. Memo trace distance can therefore be used to find nodes that can be memoized.  

\subsection{Allocation-Sensitive Trace Distance}

Keyed allocation can also greatly impact program performance, as illustrated in section \ref{sec:keyed_alloc}. However, it is possible to measure how well a program would perform if all memory is allocated in a way that no change propagation happens due to re-allocation of memory. We call this form of trace distance \textit{allocation-sensitive trace distance}. For allocation-sensitive trace distance, we use node tags without the node id. Additionally, we ignore all modifiables when comparing node tags. This also includes modifiables contained in the argument list of memo tags and modifiables contained in data structures introduced by the user.

The result is, informally speaking, that we simply ignore all differences that are induced by re-creation of modifiables.  

For two DDGs, $A$ and $B$, we denote allocation-sensitive trace distance as $\delta_a(A, B)$. 

\section{Example: Different Trace Distances for Map}
\label{sec:trace_distance_unmemoized_map}

For an illustration of our trace distances, we consider an unmemoized version of the map function introduced in section \ref{sec:simple_example}. For an unmemoized version, we are able to take the code from listing \ref{code:map_example} and remove the memoizer from the code. The resulting code can be seen in listing \ref{code:unmemoized_map_example}. While no longer memoized, the code still forms a recursive map function that supports change propagation. For any call that is re-executed, however, all sub calls are also re-executed, leading to unnecessary overhead.

\begin{figure}
\begin{lstlisting}[frame=single,basicstyle=\ttfamily,numbers=left,mathescape=true]
def map[U]
  (mapper: T => U)
  (implicit c: Context):
    Mod[SimpleList[U]] = {
  mod {
    read(currentElem) {
      case null => write(null)
      case elem => {
        val newValue = mapper(elem.value)
        val newNext = elem.next.map(mapper)
        write(new SimpleList(newValue, newNext))
      }
    }
  }
}
\end{lstlisting}
\caption{A unmemoized map function implemented on top of TBD}
\label{code:unmemoized_map_example}
\end{figure}

We now execute this function with the input list $(1, 2, 3, 4, 5)$ and a mapping function that multiplies every element by two, like before. The DDG of the execution is shown in figure \ref{fig:unmemoized_map_tbd_ddg}. As we can see, the DDG is similar to the DDG explained in section \ref{sec:tbd_ddgs}, but lacking memo nodes. 


%generated using: 
%bin/visualization.sh -a smmap -t manual -i 1  
%u 1 1
%a 2 2
%a 3 3
%a 4 4
%a 5 5
%p

\begin{figure}
\centering
\begin{tikzpicture}[font=\sffamily,very thick,level/.style={sibling distance=40mm, level distance=10mm}]
\tikzstyle{every node}=[font=\small]
\node [root, label={0:1.root}, label={350:}]{}
child {
  node (2)[mod, label={0:2.mod}, label={350:(d.11, f.6)}]{}
  child {
    node (3)[read, label={0:3.read (d.0, (1,1), f.5)}]{}
    child {
      node (4)[mod, label={0:4.mod}, label={350:(d.22, f.6)}]{}
      child {
        node (5)[read, label={0:5.read (d.1, (2,2), f.5)}]{}
        child {
          node (6)[mod, label={0:6.mod}, label={350:(d.23, f.6)}]{}
          child {
            node (7)[read, label={0:7.read (d.2, (3,3), f.5)}]{}
            child {
              node (8)[mod, label={0:8.mod}, label={350:(d.24, f.6)}]{}
              child {
                node (9)[read, label={0:9.read (d.3, (4,4), f.5)}]{}
                child {
                  node (10)[mod, label={0:10.mod}, label={350:(d.25, f.6)}]{}
                  child {
                    node (11)[read, label={0:11.read (d.4, (5,5), f.5)}]{}
                    child {
                      node (12)[mod, label={0:12.mod}, label={350:(d.26, f.6)}]{}
                      child {
                        node (13)[read, label={0:13.read}, label={350:(d.5, null, f.5)}]{}
                        child {
                          node (14)[write, label={0:14.write}, label={350:(d.26, null)}]{}
                        }
                      }
                    }
                    child {
                      node (15)[write, label={0:15.write}, label={350:(d.25, (5,10))}]{}
                    }
                  }
                }
                child {
                  node (16)[write, label={0:16.write}, label={350:(d.24, (4,8))}]{}
                }
              }
            }
            child {
              node (17)[write, label={0:17.write}, label={350:(d.23, (3,6))}]{}
            }
          }
        }
        child {
          node (18)[write, label={0:18.write}, label={350:(d.22, (2,4))}]{}
        }
      }
    }
    child {
      node (19)[write, label={0:19.write}, label={350:(d.11, (1,2))}]{}
    }
  }
};
\end{tikzpicture}
\caption{DDG of the unmemoized $map$ function with input $(1, 2, 3, 4, 5)$.}
\label{fig:unmemoized_map_tbd_ddg}
\end{figure}

If we now change the second input value from $2$ to $6$ and propagate the changes, almost the whole program is re-executed. This can be seen in figure \ref{fig:unmemoized_map_tbd_ddg_changepropagation}. The re-executed part of the program is highlighted in red. Notice that all re-executed nodes also have new node-ids. Therefore, the pure trace distance $\delta_p$ is $28$ for this pair of executions, because $14$ nodes were added, and $14$ nodes were removed. 

The memo-sensitive trace distance $\delta_m$ for this example is only $20$. The reason is that the read nodes that were not changed are not counted. The mod and write nodes, even unnecessarily executed, are counted, because the modifiables were re-allocated and therefore changed. The counted nodes are highlighted in figure \ref{fig:unmemoized_map_tbd_ddg_changepropagation} using a blue color. This indicates that the corresponding read calls did not change and therefore did not have any dependencies to the read call that had changed. Using this hint, we could try to place a $memo$ call before each read, which is  done in our memoized code from listing \ref{code:map_example}. The memoized version would have a pure trace distance of $4$ for this input, because $2$ nodes would be re-executed. This case can be seen for a similar example in figure \ref{fig:map_change_ddg_memo}. 

Allocation-sensitive trace distance, $\delta_a$, is also $4$. The reason is that allocation-sensitive trace does not count nodes that only differ by their modifiables. Those nodes, however, are the nodes that cause the high trace distance when applying memo-sensitive trace distance. The counted nodes are highlighted in green.  

\begin{figure}
\centering
\begin{tikzpicture}[font=\sffamily,very thick,level/.style={sibling distance=40mm, level distance=10mm}]
\tikzstyle{every node}=[font=\small]
\node [root, label={0:1.root}, label={350:}]{}
child {
  node (2)[mod, label={0:2.mod}, label={350:(d.11, f.6)}]{}
  child {
    node (3)[read, label={0:3.read (d.0, (1,1), f.5)}]{}
    child {
      node (20)[mod, label={0:20.mod}, label={350:(d.22, f.6)}]{}
      child {
        node (21)[read, label={0:21.read (d.1, (2,6), f.5)}]{}
        child {
          node (22)[mod, label={0:22.mod}, label={350:(d.23, f.6)}]{}
          child {
            node (23)[read, label={0:23.read (d.2, (3,3), f.5)}]{}
            child {
              node (24)[mod, label={0:24.mod}, label={350:(d.24, f.6)}]{}
              child {
                node (25)[read, label={0:25.read (d.3, (4,4), f.5)}]{}
                child {
                  node (26)[mod, label={0:26.mod}, label={350:(d.25, f.6)}]{}
                  child {
                    node (27)[read, label={0:27.read (d.4, (5,5), f.5)}]{}
                    child {
                      node (28)[mod, label={0:28.mod}, label={350:(d.26, f.6)}]{}
                      child {
                        node (29)[read, label={0:29.read}, label={350:(d.5, null, f.5)}]{}
                        child {
                          node (30)[write, label={0:30.write}, label={350:(d.26, null)}]{}
                        }
                      }
                    }
                    child {
                      node (31)[write, label={0:31.write}, label={350:(d.25, (5,10))}]{}
                    }
                  }
                }
                child {
                  node (32)[write, label={0:32.write}, label={350:(d.24, (4,8))}]{}
                }
              }
            }
            child {
              node (33)[write, label={0:33.write}, label={350:(d.23, (3,6))}]{}
            }
          }
        }
        child {
          node (34)[write, label={0:34.write}, label={350:(d.22, (2,12))}]{}
        }
      }
    }
    child {
      node (19)[write, label={0:19.write}, label={350:(d.11, (1,2))}]{}
    }
  }
};
\begin{pgfonlayer}{background}
\fill[lightred] \convexpath{21,34,31,30,28,26,24,22}{24pt};
\fill[lightblue,opacity=1] \convexpath{21,34,22}{16pt};
\fill[lightblue,opacity=1] \convexpath{24, 33}{16pt};
\fill[lightblue,opacity=1] \convexpath{26, 32}{16pt};
\fill[lightblue,opacity=1] \convexpath{28, 31}{16pt};
\fill[lightblue,opacity=1] \convexpath{34, 33, 32, 31, 30, 31}{16pt};
\fill[lightgreen,opacity=1] \convexpath{21, 34}{8pt};
\end{pgfonlayer}
\end{tikzpicture}
\caption{DDG of the unmemoized $map$ function after the second value was updated to $6$}
\label{fig:unmemoized_map_tbd_ddg_changepropagation}
\end{figure}


Note that it is not meaningful to introduce keyed allocation in this example, since memoization alone is sufficient. It is therefore advisable to inspect a run using memo-sensitive trace distance before consulting allocation-sensitive trace distance. In general, it can be said, that a trace distance that utilizes larger subsets of node properties should be consulted before a trace distance that uses smaller subsets of node properties. This way, erroneous assumptions can be avoided.

It should also be noted that it is dangerous to generalize from one example or run to all applications of a certain program or function. Consulting trace distances, however, might be very useful for finding potential performance bottlenecks for certain inputs. 

We will discuss the application of trace distance for more complex examples in \ref{ch:eval}.

\chapter{Implementation}

\begin{figure}
\begin{center}
\includegraphics[scale=0.7]{uml/Framework.pdf}
\end{center}
\caption{Overview over the analyzer framework architecture}
\label{fig:testing_framework}
\end{figure}

To test the concept of trace distance, we created an analyzer framework that is capable of generating input for TBD and provides an interface for TBD programs to test. The input consists of initial data and updates that are propagated by TBD. During runtime, additional information is collected and added to the DDGs, then the DDG of each execution is copied, since TBD mutates its DDS during change propagation. Also, found dependencies are summarized to ease further processing. The DDGs, results and input of each run are stored for analysis. Modules to process the results can be added. Existing modules include visualization of DDGs, comparing DDGs of different runs, calculating trace distance, and aggregation of statistics. This basic architecture of this analyzer framework is illustrated in Figure \ref{fig:testing_framework}.

It is important to note that in the current state, the framework uses ordered sets implemented as linked lists as input.

\section{Input Generation and Test Program Interface}

For enabling flexible input generation during tests, an interface for test case generators has been implemented. The architecture of this interface, including existing test case generators, can be seen in \ref{fig:input_generation}. The figure also illustrates the interface that invokes programs that should be tested. 

\begin{figure}
\begin{center}
\includegraphics[scale=0.7]{uml/TestBase.pdf}
\end{center}
\caption{Overview over the input generation}
\label{fig:input_generation}
\end{figure}

\subsection{Adjustable}

The interface\footnote{Interface or Trait in Scala} $Adjustable$ originates from TBD. A program that should be executed by TBD has to implement this interface. The $run$ method of the Interface $Adjustable$ takes a parameter, the program input, and returns a parameter, the program output. The types of both parameters are bound to type parameters. Both types are usually based on modifiables. 

\subsection{TestAlgorithm}

The interface $TestAlgorithm$ implements the interface $Adjustable$ and represents a program or algorithm that should be tested using this framework. The $TestAlgorithm$ interface provides three methods. The method $getExpectedResult$ is responsible for computing the program output in a non-incremental way. Therefore, instead of using a type based on modifiables, the method receives the input as a map, containing all keys and values. The method $getResult$ is responsible for converting the output given by the $run$ method into some output representation without modifiables. If this converted output does not equal the value returned from the method $getExpectedResult$ for the same input, the test fails. The $getInput$ method has to return an instance of the abstract class $tbd.Input$, that manages input for $TBD$ programs. This class has methods to add, delete or update modifiables and a getter that returns the input in the desired representation, for example a linked list. 

\subsection{ExperimentSource}

The interface $ExperimentSource$ represents a whole test, including input generation, that can be executed in our testing framework. The interface provides a single method, $setExperimentListener$, that associates an instance of $ExperimentSink$ with the instance. Whenever a single step of the experiment finishes, the instance of $ExperimentSink$ will be notified with the results. $ExperimentSink$ is a interface that defines a single method that accepts the result of a run, the DDG and the input data as parameters. All of the output modules that will be described later implement the interface $ExperimentSink$.

\subsection{TestBase}

The abstract class $TestBase$ also represents a test, but also implements some useful methods that can be used to create tests. The boolean $checkResults$ indicates whether run results should be checked by the test, the integer $initialSize$ indicates the size of the first input set, and $algorithm$ is the algorithm or program to test. 

The method $addValue$ adds a modifiable to the input using the given key and value. The method $removeValue$ removes the modifiable with the given key from the input. 
The method $updateValue$ updates the modifiable with the given key to the given new value. 

The $run$ method, when invoked by TBD, creates the $Input$ object for the given algorithm to test. Then, the abstract $initialize$ method is called, where the test should be initialized. After that, the abstract method $step$ is called until one call to $step$ returns $false$. After each call of step, TBD is notified to propagate the changes. The corresponding $ExperimentSink$ is notified about the results of change propagation, including the current DDG and input. At the end, the abstract method $dispose$ is invoked.

\subsection{Implemented Test Generators}

The class $RandomExhaustiveTest$ implements a test that performs random actions on the input. The test will do $count$ steps. The number of mutations, including deletions and insertions, during each step will be randomly selected between $maxMutations$ and $minMutations$. 

The class $TargetedExhaustiveTest$ will perform any possible consecutive updates on the input. More specifically, for each position in the input, the test will perform random changes of all variables in all consecutive subsets starting at this location. The parameter $repetitions$ indicates how often this test should be repeated. It is not recommended to run this test with large input sets, since the time complexity of generating all possible consecutive updates is $O(n^2)$, where $n$ denotes the input size.

The class $ManualTest$ uses the command line to let the user update the input of the program. 

\section{Data Collection}

TBD holds a DDG to control change propagation. It is possible to copy and access the DDG for analytical purposes. However, the DDG introduced by TBD does not contain a compact or unique representation of called functions, as described in section \ref{sec:tbd_ddgs}. The reason is that for implementing change propagation, closures introduced by the Scala language are sufficient. Those closures, however, do not contain a definition for equality. In particular, free variables that are bound by an outer scope are difficult to handle. It might be possible, that two closures are syntactically identical, and the parameters are the same, but the results are still different due to free variables. Such a situation is illustrated in listing \ref{code:bound_free_var}. In the example, the variable $a$ inside the closure passed to the $write$ call is bound to the variable $a$ defined in line $1$. The closure is therefore dependent on the scope that contains the closure. This dependency is semantically identical to a parameter passed to the corresponding function call. 

\begin{figure}
\begin{lstlisting}[frame=single,basicstyle=\ttfamily,numbers=left,mathescape=true]
val a = 10
mod {
  write(a)
}
\end{lstlisting}
\caption{Example of a free variable that is bound from an outer scope} 
\label{code:bound_free_var}
\end{figure}

\subsection{Generation of Function Tags}

To gather function tags, we utilize \textit{Scala macros}. Scala macros are Scala programs that are invoked during compile time and can access and modify all information accessible by the compiler. Scala macros get an \textit{Abstract Syntax Tree (AST)} as input, which can be modified during macro execution as described by M. Stocker in \cite{stocker2010scala}. An AST is basically a tree representation of source code, as used internally by the Scala compiler. It is important to emphasis that an AST does not represent call trees, but trees of symbols parsed from a source file. In particular, this means that a call to a function that is defined in another file is a leave node in the AST, while an anonymous function that is contained in the same file is represented as a subtree. 

For collecting function tags, we replace the $read$, $memo$, $mod$ methods and the $apply$ method of $Modizer$ and $Memoizer$ with a macro application. The original implementations of the methods are renamed and marked as private. Also, we add a function tag as additional parameter to the private method. The macro is executed by the compiler for each call of a method that refers to a macro application. Therefore, it is possible to generate unique ids for each call and for each closure. When called, the macro examines the closure passed to the function, which is available as AST. Using the AST we find all free variables that are bound by the closure from an outer scope. To do so, we traverse the AST of the closure, to locate all references to variables. Then, for each reference, we traverse all parent nodes of the reference in the AST of the closure and check whether the variable is defined there. If so, we do not need to consider that reference, since it is not from an outer scope. Finally, we generate code that instantiates a list that contains the values of all free variables and their corresponding names as strings. Then, the generated code creates a new instance of function tag with the generated id and the list of free variables and calls the original implementation of the corresponding method with the function tag as additional parameter. The function tag is then attached to the corresponding DDG node by TBD at run time. 

\subsection{DDG Implementation}

During runtime, we receive the DDG from TBD. Since the DDG of TBD is mutable, we need to transfer it to an immutable representation that is more suitable for fast traversal. The corresponding classes are outlined in figure \ref{fig:framework_ddg}. 

\begin{figure}
\begin{center}
\includegraphics[scale=0.7]{uml/DDG.pdf}
\end{center}
\caption{Overview over the DDG data structure}
\label{fig:framework_ddg}
\end{figure}

We introduce a $Node$ class for DDG nodes. This node class holds a stack trace of the corresponding call, the node id and the tag, which holds all properties of the DDG node, including the node type. Also, we introduce an abstract $Edge$ class for DDG edges, referring to a source and a destination node. Subclasses of the $Edge$ class represent different types of edges, like $Control$, $ModWrite$, $ReadWrite$ or $FreeVar$ dependencies. $Control$ edges refer to calls, $ModWrite$ edges to allocate-write dependencies, $ReadWrite$ edges to read-write dependencies and $FreeVar$ edges to dependencies introduced by free variables bound from an outer scope. $FreeVarEdges$ also hold the name and values of those variables.

The $DDG$ class itself includes an adjacency list, a set containing all nodes and a reference to the root node of the DDG. Since the control edges of a DDG form a tree that can be traversed in a meaningful order, helper methods to find child and parents nodes exist. 

To ease the traversion of DDGs for processing, a $GraphIterator$ interface, following an iterator pattern, was introduced. Also, three implemented iterators are provided: 

\begin{enumerate}
\item The $DfsIterator$, that iterates over the graph in depth-first order and outputs each node whenever it is visited by the depth-first search. 
\item The $DfsFirstIterator$, that iterates over the graph in depth-first order and only outputs the node when it is visited by the depth-first search for the first time.
\item The $TopologicalIterator$, that iterates over the graph in depth-first order and only outputs the node when it is visited by the depth-first search for the last time.  
\end{enumerate}

\subsection{Dependency Processing}

All nodes in the DDG contain a lot of information, as described in section \ref{sec:tbd_ddgs}. For visualization purposes, however, it is useful to show dependencies between nodes as edges when displaying the DDG. 

We consult each mod, write and read node and pair them using the corresponding mod ids. This way, we can find all allocate-write and read-write dependencies and insert them into the DDG as $ModWrite$ or $ReadWrite$ edges.

For each node that contains a function tag that has bound free variables, we traverse all call parents until we reach a node where the corresponding variable is no longer available. We also add a $FreeVar$ edge to the DDG here, which symbolizes a free-variable dependency. If even the root node of the program contains a free variable that is bound from an outer scope, the variable originates from outside the program and can be treated as constant. 

For finding allocate-read and allocate-write dependencies, the DDG can be traversed in topological order, while the ids of allocated modifiables and the corresponding nodes are stored in a hash map. The search for free-variable dependencies can be implemented by inserting a $FreeVar$ edge from each node that has free variables to its parent and then performing a path compression, by fusing adjacent dependency edges that refer to the same variable. Both approaches result in a runtime of $O(n)$, where $n$ is the number of nodes.

\subsection{Trace Distance}

The computation of trace distance is straight forward, as already described in section \ref{ch:calc_trace_distance}. A greedy matching can be implemented using two sets and a for loop. Furthermore, we remember what nodes are removed or added to the DDGs to visualize them later.

Since we have multiple valid definitions of node equality for calculating trace distance, we introduce a wrapper class that extracts the relevant properties from a node and implements the $equals$\footnote{A Java or Scala specific method that is available for each object and is used to check whether two objects are equal. The method can be overloaded.} and $hashCode$\footnote{A Java or Scala specific method that is available for each object and generates a specific integer code for the object, that is used to sort the object into hash sets. If two objects are equal, their hash codes should also be equal. This Method can be overloaded.} method based on those properties. We wrap each node before calculating trace distance. This way we can simply rely on Scala's collection libraries.

For implementing allocation sensitive trace distance, we introduce a thread static variable\footnote{A variable that is only set for the current thread.} that modifies the $equals$ and $hashCode$ methods of mods. If the variable is set, $equals$ always returns true, and $hashCode$ always returns $0$, resulting in all mods being recognized as equal and effectively ignoring them when comparing objects. 

\section{Framework Output}

All the data collected from TBD needs to be visualized in a meaningful format. Especially dependencies and the comparison of DDGs can be shown best by using a graphical representation. Also, it has proven useful to automatically aggregate data from multiple tests to generate diagrams. 

\subsection{DDG Visualization}

We visualize DDGs using a small GUI implemented in Swing\footnote{A lightweight UI framework that was introduced for Java}. A screenshot of this GUI is shown in figure \ref{fig:ddg_vis_ui}. 

\begin{figure}
\begin{center}
\includegraphics[scale=0.7]{screens/DDGVis.png}
\end{center}
\caption{Screenshot of the DDG visualizer}
\label{fig:ddg_vis_ui}
\end{figure}

The drop down list at the top can be used to choose from different runs. The runs are identified by a increasing counter and the corresponding input, which are both shown in the list. Below, the selected DDG is shown in the same format as described in section \ref{sec:tbd_ddgs}. For selected nodes, dependencies are shown. Red, solid arrows indicate a write-read or allocate-write dependency. The exact dependency can be inferred from the types of the selected nodes. The dashed arrows correspond to dependencies introduced by free variables. The currently selected node is the node from which the dependency edges originate. 

Below the DDG, the description of the currently selected node, a read node, is shown. In the first line, we see the call location of the current node, including method name, file, line number and TBD id. The method name of $liftedTree1$ indicates that this node was re-executed during change propagation. In the second line we see the node type and the parameters of the call. In this case, the modifiable with the id $d.26$ and the value $null$ were read. Below this information, details regarding the closure can be found. The identifier of the closure is $76$ and there are four variables, $round$, $next$, $hasher$ and $memo$, bound from an outer scope. The colors of the text correspond to the colors of the arrows shown in the visualizer. The last part summarizes variables that are bound from outside the program.  

The button in the top-right corner labeled $L$ dumps a \LaTeX \hspace{0.5mm} representation of the current DDG to the standard output stream. The \LaTeX \hspace{0.5mm} representation can be rendered using the \LaTeX \hspace{0.5mm} package \textit{tikz}. 

\subsection{DDG Comparison}

When analyzing trace distance, it is also useful to investigate which nodes were removed or added during change propagation. We therefore introduce a GUI to select and compare arbitrary runs. This GUI is shown in figure \ref{fig:ddg_diff_ui}. 

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{screens/DiffView.png}
\end{center}
\caption{Screenshot of the DDG comparer}
\label{fig:ddg_diff_ui}
\end{figure}

The GUI contains of two DDG visualizers, where DDGs from two different runs can be selected. At the bottom, there exists a drop down-list to select the type of trace distance to apply. The results of the trace distance calculation are shown below. Added nodes correspond to nodes that are not in the left DDG, but in the right one. Removed nodes correspond to nodes that are in the right DDG, but not in the left one. Inside the visualizer, removed nodes are marked with a yellow border, added nodes are marked with a red border.

\subsection{Table Export}

It is possible to export the results of multiple trace distance calculations as summarized table. The table works best for the test cases generated by $TargetedExhaustiveTest$. One axis indicates the index where input updates started, the other indicates the length of the update. The values stored in the table show the trace distance of the DDG after the update compared to the DDG before the update and therefore indicate the overhead of change propagation for the given update. 

Table \ref{tbl:introducing_example} shows an example of such an exported table for a naive map operation without memoization and an input size of $10$. In this example, We can recognize that not only the size, but also the starting index of the update linearly increased the overhead of change propagation. Such a result usually indicates that memoization is either missing or not working, since small updates at the beginning of the list have the same overhead as updates that alter the whole list. 

\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|}
\hline
\diaghead{\theadfont Diag ColumnmnHead II}%
{Update\\Length}{Start\\Index} & 1  & 2  & 3  & 4  & 5  & 6  & 7  & 8  & 9  & 10 \\ \hline
1  & 42 & 38 & 34 & 30 & 26 & 22 & 18 & 14 & 10 & 6  \\ \hline
2  & 42 & 38 & 34 & 30 & 26 & 22 & 18 & 14 & 10 &    \\ \hline
3  & 42 & 38 & 34 & 30 & 22 & 22 & 18 & 14 &    &    \\ \hline
4  & 42 & 38 & 34 & 30 & 26 & 22 & 18 &    &    &    \\ \hline
5  & 42 & 38 & 34 & 30 & 26 & 22 &    &    &    &    \\ \hline
6  & 42 & 38 & 34 & 30 & 26 &    &    &    &    &    \\ \hline
7  & 42 & 38 & 34 & 30 &    &    &    &    &    &    \\ \hline
8  & 42 & 38 & 34 &    &    &    &    &    &    &    \\ \hline
9  & 42 & 38 &    &    &    &    &    &    &    &    \\ \hline
10 & 42 &    &    &    &    &    &    &    &    &    \\ \hline
\end{tabular}
\caption{Pure trace distance induced by different successive updates to the input of unmemoized map, by starting position and update length}
\label{tbl:introducing_example}
\end{table}



\section{Command Line Interface}

The analyzer framework can be controlled using a command line interface. The interface can be used to select test cases, algorithms to run, or program output types. Also, various parameters can be set. The complete list of parameters can be found in table \ref{tbl:commands}.

\begin{table}[h]
\centering
\begin{tabular}{|l|p{10cm}|}
\hline
-a NAME  & Selects the program to run during testing. NAME has to refer to the name of the algorithm as given in the $Main$ class of the analyzer framework. \\ \hline
-i COUNT & Sets the count of elements in the initial input data.\\ \hline
-t NAME  & Selects the test case generator: $random$, $manual$ or $exhaustive$.\\ \hline
-o NAME  & Sets the output format for the results: $visualizer$, $diff$ or $chart2d$.\\ \hline
-r & When set, the testing framework will skip to check whether the program output is correct or not. \\ \hline
-c COUNT & When $random$ is selected as test generator, sets the count of update rounds to generate. \\ \hline
-k COUNT & When $random$ is selected as test generator, sets the minimal number of updates per update round. \\ \hline
-p COUNT & When $random$ is selected as test generator, sets the maximal number of updates per update round. \\ \hline
-d NAME & When $chart2d$ is selected as output, sets the type of trace distance to use, either $pure$, $memo$ or $alloc$. \\ \hline
\end{tabular}
\caption{Command line parameters for the analyzer framework}
\label{tbl:commands}
\end{table}

\chapter{Evaluation}
\label{ch:eval}

We evaluate our implementation of trace distance and the concept of pure and allocation-sensitive trace distance. For an evaluation, we apply trace distance to implementations of the algorithms $map$, $reduce$ and quicksort. We will discuss each implementation, since trace distance and change propagation overhead are dependent on implementation-specific decisions. An example would be allocating a modifiable inside a read, where it is avoidable, or reading variables before they are actually needed. For each program, we will explain the expected time complexity and behavior, as described by U. Acar in \cite{Acar2005thesis}. Then, we will show that our implementation of trace distance delivers equal results for program runs. In particular, we will show that allocation sensitive trace distance for an unmemoized algorithm will be similar to pure trace distance for a memoized version of the same algorithm, for an equal input mutation. 

As experiment setup, we generate random updates on all possible successive positions on a initial input set with size $50$\footnote{This input size was chosen due to hardware limitations of our testing machine.} for the mentioned algorithms. In other words, for each start index $i$ of the input set, we will perform random updates of each possible length $u$ on the input set. The results of this exhaustive tests are shown using surface plots, where the $i$ and $u$ axes correspond to the start index and the length of the update, respectively, and the value on the vertical axis corresponds to the trace distance. We only include updates of values, but no insertions or deletions into the input set. The reason is that tests cases would get a lot bigger and more complicated. While insertions and deletions are very interesting from the perspective of algorithm design, they are not of particular important for an evaluation of trace distance. Each test run is repeated five times, the results are averaged. 

We do not evaluate the implementation of memo-sensitive trace distance here, since memo-sensitive trace distance does not correspond to a certain version of a program. Memo-sensitive trace distance is merely a metric between pure and allocation-sensitive trace distance, that can be used to find nodes that are easily memoizable. 

\section{Map}

The unmemoized and memoized versions of $map$ have been discussed thoroughly in section \ref{sec:tbd_basic_example} and section \ref{sec:trace_distance_unmemoized_map}. For an unmemoized version of $map$, we expect a change propagation overhead that depends not only on the update size, but also on the starting position of the update, since $map$ will also be executed for all successive elements. Formally, this means that if the element with index $i$ was updated, change propagation overhead will be at least as high as for an update of size $n-i$, where $n$ denotes the input size. A plot using pure trace distance can be seen in plot \ref{plot:unmemoized_map_pure}. We can see that even for small updates, we have high overhead, depending on the position of the update. 

\begin{figure}
\centering
\begin{tikzpicture}
\begin{axis}[xlabel=$i$,ylabel=$u$,zlabel=$\delta_p$]
\addplot3[surf]
table{data/MapPure.txt};
\end{axis}
\end{tikzpicture}
\caption{Pure trace distance for updates of a unmemoized version of map. $i$ corresponds to the start update, $u$ to the length of the update.}
\label{plot:unmemoized_map_pure}
\end{figure}


\begin{figure}
\centering
\begin{tikzpicture}
\begin{axis}[xlabel=$i$,ylabel=$u$,zlabel=$\delta_p$]
\addplot3[surf]
table{data/MemoMapPure.txt};
\end{axis}
\end{tikzpicture}
\caption{Pure trace distance for updates of a memoized version of map}
\label{plot:memoized_map_pure}
\end{figure}


\begin{figure}
\centering
\begin{tikzpicture}
\begin{axis}[xlabel=$i$,ylabel=$u$,zlabel=$\delta_a$]
\addplot3[surf]
table{data/MapAlloc.txt};
\end{axis}
\end{tikzpicture}
\caption{Allocation sensitive trace distance for updates of a unmemoized version of map}
\label{plot:unmemoized_map_alloc}
\end{figure}

For the memoized version of $map$, we expect a change propagation overhead that is solely dependent on the update size. In particular, the overhead is independent of the update position. We can see this behavior in plot \ref{plot:memoized_map_pure}, that was generated from runs of the memoized version of $map$ using pure trace distance. If we apply allocation sensitive trace distance to the unmemoized version of $map$, we get similar results, as illustrated in plot \ref{plot:unmemoized_map_alloc}. The trace distance for each run is about one third larger for the memoized version. The reason is that the memoized version has additional memo nodes in the DDG, that are also counted. 

We can therefore say that in this example, allocation sensitive trace distance successfully predicted how our unmemoized $map$ algorithm would perform if it was memoized and all allocations were optimal. Memoization alone is sufficient for implementing an efficient map, since it will stop change propagation before mods are re-created. 

\section{Linear Reduce}

For reduce, we discussed a naive linear version and a tree-like version in theory in section \ref{sec:more_complex_example}. We did not, however, discuss reduce as a TBD program. A recursive implementation of a linear reduce can be seen in figure \ref{code:linear_reduce}. We create a function, $linearReduce$, that allocates the modifiable for the result of the reduce operation and calls a recursive, internal reduce function, $linearReduceInternal$ for the head of the list. The function $linearReduce$ accepts two parameters, a function parameter that combines two values to some output value, and a mod holding an initial value. The function $linearReduceInternal$ also accepts the function parameter, and also a modifiable holding the result of previous reduce operations. If we reach the end of the list, this previous result is written as final result. If there are still elements left, the current element is combined with the previous result and the function is called for the next element. 


\begin{figure}
\begin{lstlisting}[frame=single,basicstyle=\ttfamily,numbers=left,mathescape=true]
def linearReduce[OutputElement]
  (reducer: (Element, OutputElement) => OutputElement, 
  initialValue: Mod[OutputElement])
  (implicit c: Context): Mod[OutputElement] = {
    mod {
      naiveLinearReduceInternal(reducer, initialValue)
    }
}

private def linearReduceInternal[OutputElement]
  (reducer: (Element, OutputElement) => OutputElement, 
  previous: Mod[OutputElement])
  (implicit c: Context): Changeable[OutputElement] = {
  read(elem) {
    case null => {
      read(previous) {
        write(_)
      }
    }
    case elem => {
      val akku = mod {
        read(previous) {
          case p => write(reducer(elem.value, p))
        }
      }

      elem.next.linearReduceInternal(reducer, akku)
    }
  }
}
\end{lstlisting}
\caption{A linear reduce function implemented on top of TBD}
\label{code:linear_reduce}
\end{figure}

As discussed in chapter \ref{sec:more_complex_example}, we expect a change propagation overhead similar to the overhead of an unmemoized map. Updates that are done close to the beginning of the input list will cause updates for all successive list elements. This behavior can also be seen in plot \ref{plot:reduce_pure}. Plot \ref{plot:reduce_pure} shows the pure trace distance for all possible successive updates for this function. As already discussed, this behavior can not be avoided using memoization. A similar result result can also be seen in plot \ref{plot:reduce_alloc}, generated using allocation sensitive trace distance. The change propagation overhead is smaller for small updates, since some updates to mod and read nodes can be omitted using memoization and keyed allocation. This, however, does not change asymptotic time complexity and introduces a lot of overhead due to additional memo nodes. We construct a memoized version by placing a $memo$ call before the current element is read in line $14$ and by supplying the key of the element as key to the $mod$ function in line $21$. A plot of pure trace distance for the version including memoization and keyed allocation can be found in figure \ref{plot:memo_reduce_pure}. The trace distance for a complete update of the input is higher, since the DDG now contains additional memo nodes. Aside from that, the results for allocation sensitive trace distance and a non-memoized version and pure trace distance of a version with keyed allocation and memoization are similar. 

\begin{figure}
\centering
\begin{tikzpicture}
\begin{axis}[xlabel=$i$,ylabel=$u$,zlabel=$\delta_p$]
\addplot3[surf]
table{data/LinearReducePure.txt};
\end{axis}
\end{tikzpicture}
\caption{Pure trace distance for updates of a linar form of reduce}
\label{plot:reduce_pure}
\end{figure}

\begin{figure}
\centering
\begin{tikzpicture}
\begin{axis}[xlabel=$i$,ylabel=$u$,zlabel=$\delta_a$]
\addplot3[surf]
table{data/LinearReduceAlloc.txt};
\end{axis}
\end{tikzpicture}
\caption{Allocation sensitive trace distance for updates of a linar form of reduce}
\label{plot:reduce_alloc}
\end{figure}

\begin{figure}
\centering
\begin{tikzpicture}
\begin{axis}[xlabel=$i$,ylabel=$u$,zlabel=$\delta_p$]
\addplot3[surf]
table{data/MemoLinearReducePure.txt};
\end{axis}
\end{tikzpicture}
\caption{Pure trace distance for updates of a linar form of reduce with memoization and keyed allocation}
\label{plot:memo_reduce_pure}
\end{figure}

\section{Tree-like Reduce}

A non-randomized tree-like reduce was theoretically discussed in section \ref{sec:more_complex_example}. An implementation of the tree-like reduce can be seen in listing \ref{code:tree_reduce}. We define a helper function, $reducePairs$, that reads two elements from a list and conbines them to one element using the supplied function. If applicable, the function calls itself recursively for the next element. When finished, the function returns a list that contains all combined elements. For an input list, $reducePairs$ is called by the $treeReduceInternal$ method, which also calls itself again with the result. The $treeReduceInternal$ method continues recursively until the input list is reduced to a single element. Then, the value of that single element is read and returned. The method $treeReduce$ only allocates the memory for the result and invokes $treeReduceInternal$.

\begin{figure}
\begin{lstlisting}[frame=single,basicstyle=\ttfamily,numbers=left,mathescape=true]
def treeReduce
  (reducer: (Element, Element) => Element)
  (implicit c: Context): Mod[Element] = {
    mod {
      treeReduceInternal(reducer)
    }
}

def treeReduceInternal
  (reducer: (Element, Element) => Element)
  (implicit c: Context): Changeable[Element] = {
    read(elem) {
      case e => {
        read(e.next) {
          case null => write(e.value)
          case next => {
            val newList = mod {
              elem.reducePairs(reducer)
            }
            newList.treeReduceInternal(reducer)
          }
        }
      }
    }
}

def reducePairs
  (reducer: (Element, Element) => Element)
  (implicit c: Context): Changeable[SimpleList[Element]] = {
    read(elem) {
      case null => write(null)
      case elem => {
        read(elem.next) {
          case null => write(elem)
          case next => {
            val newNext = mod {
              next.next.reducePairs(reducer)
            }
            val newValue = reducer(elem.value, next.value)
            val newElement = new SimpleList[Element]
              (newValue, elem.key, newNext)
            write(newElement)
          }
        }
      }
    }
}

\end{lstlisting}
\caption{A tree-like reduce function implemented on top of TBD}
\label{code:tree_reduce}
\end{figure}

For this unmemoized version, we expect change propagation overhead of $O(n)$ in the worst case. Like for an unmemoized map, the overhead is dependent on the update position, rather than the length of the update. The reason is that if an an input element to the $reducePairs$ method changes, the corresponding reduced element in the output list changes, too. Since this version does not include any memoization, all successive calls to $reducePairs$ are also re-executed. We can see that behavior in figure \ref{plot:tree_reduce_pure}, that plots pure trace distance for updates to a unmemoized version of tree-reduce. Like for previous examples, the change propagation time can be very high for small updates, provided they are close to the beginning of the input list. 

\begin{figure}
\centering
\begin{tikzpicture}
\begin{axis}[xlabel=$i$,ylabel=$u$,zlabel=$\delta_p$]
\addplot3[surf]
table{data/TreeReducePure.txt};
\end{axis}
\end{tikzpicture}
\caption{Pure trace distance of a tree-like reduce operation}
\label{plot:tree_reduce_pure}
\end{figure}

When we add memoization and keyed allocation to the tree-like reduce operation, we memoize each call to $reducePair$ with the current element as argument. Also, we use the current element as key for allocating the result of the reduce operation. Those operations are very similar to those done with $map$ in section \ref{sec:tbd_memo_map}. Due to this changes, we expect change propagation time to lie in the complexity class of $O(log(n))$, as discussed in section \ref{sec:more_complex_example}. Also we expect change propagation overhead to be only dependent on update size. 

Figure \ref{plot:tree_reduce_alloc} shows the allocation sensitive trace distance for an unmemoized version of tree-reduce. We can clearly see that the trace distance in this example depends on the update size, not the position. This also holds for the memoized version. The plot of pure trace distance for the memoized version with keyed allocation can be seen in figure \ref{plot:memo_tree_reduce_pure}. All three plots show the results we expected for this program. 

\begin{figure}
\centering
\begin{tikzpicture}
\begin{axis}[xlabel=$i$,ylabel=$u$,zlabel=$\delta_a$]
\addplot3[surf]
table{data/TreeReduceAlloc.txt};
\end{axis}
\end{tikzpicture}
\caption{Allocation sensitive distance of a tree-like reduce operation}
\label{plot:tree_reduce_alloc}
\end{figure}



\begin{figure}
\centering
\begin{tikzpicture}
\begin{axis}[xlabel=$i$,ylabel=$u$,zlabel=$\delta_p$]
\addplot3[surf]
table{data/MemoTreeReducePure.txt};
\end{axis}
\end{tikzpicture}
\caption{Pure distance of a memoized tree-like reduce operation}
\label{plot:memo_tree_reduce_pure}
\end{figure}

\section{Quicksort}

Our final test is an implementation of quicksort on linked list. This implementation works by using the first element of a list as pivot element, then splitting the list in a part greater than, and in a part smaller than the pivot element. The quicksort is called recursively until the input list consists only of a single element. 

To split a list into two parts, we define a $split$ function that accepts a function parameter that indicates whether an element should be in the first list or the second list. The $split$ function can be seen in listing \ref{code:split_filter}. Internally, the $split$ function uses two applications of a $filter$ function. That function takes a higher order function and returns an output list that contains all lists for which the higher order function returned true. Filter uses an internal function $filterInternal$, that is called recursively. It should be noted here that $filter$ and $filterInternal$ already allocate memory for each output before the corresponding input element is read. It might happen that we allocate memory, and do not write that memory until we have discarded a lot of input elements. Due to this property, it is not possible to take advantage of keyed allocation, since we do not know a meaningful key where we allocate the memory. It is not possible to create the mod where we write it, due to the enforcement of TBD that the closure passed to a read has to return an instance of $Changeable$. 

\begin{figure}
\begin{lstlisting}[frame=single,basicstyle=\ttfamily,numbers=left,mathescape=true]
def split
  (predicate: Element => Boolean)
  (implicit c: Context):
    (Mod[SimpleList[Element]], Mod[SimpleList[Element]]) = {
  (elem.filter(x => predicate(x)), 
   elem.filter(x => !predicate(x)))
}

def filter
  (predicate: Element => Boolean)
  (implicit c: Context):
    Mod[SimpleList[Element]] = {

  mod {
    elem.filterInternal(predicate)
  }
}

def filterInternal
  (predicate: Element => Boolean)
  (implicit c: Context):
    Changeable[SimpleList[Element]] = {

  read(elem) {
    case null => {
      write(null)
    }
    case elem =>

      if(predicate(elem.value)) {
        val newNext = mod {
          elem.next.filterInternal(predicate)
        }
        write(new SimpleList(elem.value, elem.key, newNext))
      } else {
        elem.next.filterInternal(predicate)
      }
  }
}

\end{lstlisting}
\caption{A $split$ function that splits an input list into two lists using a predicate}
\label{code:split_filter}
\end{figure}

The implementation of quicksort can be seen in listing \ref{code:quicksort}. The $quicksort$ function allocates memory for the first output element and initializes the last element of the list. Then, $quicksortInternal$ is called. The $quicksortInternal$ function is responsible for recursively sorting a linked list and appending the supplied element $tail$ to the list. For each input list, the $quicksortInternal$ function reads the first element of the list. Then, the function splits the rest of the list in two parts, one containing all elements greater, and one containing all elements smaller than the first element. Finally, those lists are sorted using a recursive call of $quicksortInternal$ and then concatenated. The part to add to the end of a list is supplied as $tail$ to each recursive call.

\begin{figure}
\begin{lstlisting}[frame=single,basicstyle=\ttfamily,numbers=left,mathescape=true]
def quickSort
  (comperator: (Element, Element) => Boolean)
  (implicit c: Context): Mod[SimpleList[Element]] = {

    val tail = mod { write[SimpleList[Element]](null) }

    mod {
      quickSortInternal(comperator, tail)
    }
}

def quickSortInternal
  (comperator: (Element, Element) => Boolean,
   tail: Mod[SimpleList[Element]])
  (implicit c: Context): Changeable[SimpleList[Element]] = {

    read(elem) {
      case null => read(tail) {write(_)}
      case e => {
        val (smaller, greater) = {
          e.next.split(x => comperator(x, e.value))
        }
        val greaterSorted = mod { 
          greater.quickSortInternal(comperator, tail) 
        }
        val middle = mod { 
          write(new SimpleList(e.value, e.key, greaterSorted)) 
        }
        smaller.quickSortInternal(comperator, middle)
      }
    }
}

\end{lstlisting}
\caption{A quicksort implementation on top of TBD}
\label{code:quicksort}
\end{figure}

For quicksort we expect a time complexity of $O(n * log(n))$, where $n$ is the length of the input list. This also holds for updates, as we can see in figure \ref{plot:quicksort_pure}. The plot shows pure trace distance for updates on the unmemoized version of quicksort. We can clearly see that the change propagation overhead does not only depend on the input size, but also on the length of the input, which was expected. 

\begin{figure}
\centering
\begin{tikzpicture}
\begin{axis}[xlabel=$i$,ylabel=$u$,zlabel=$\delta_p$]
\addplot3[surf]
table{data/QuicksortPure.txt};
\end{axis}
\end{tikzpicture}
\caption{Pure trace distance for updates of a quicksort}
\label{plot:quicksort_pure}
\end{figure}

U. Acar did a comprehensive analysis of quicksort in \cite{Acar2005thesis}. According to his writing, the expected time complexity for updates at the beginning of the list is within $O(n)$ where $n$ denotes the input size. For updates at uniformly random positions, the expected time complexity is within $O(log(n))$. This leads to the conclusion that for a correctly memoized version, the input position is not going to influence change propagation time if the update is not too close to the beginning of a list. We can see similar results in figure \ref{plot:quicksort_alloc}, that plots allocation sensitive trace distance for an unmemoized version of quicksort. 

For memoizing quicksort, using a memoized filter function is sufficient. We memoize $filter$ by placing a $memo$ call around the $mod$ call and use the current element as argument to the $memo$ function. Since the $filter$ function has to initialize a memoizer for that purpose, we also have to memoize the first call to $filter$ so the memoizer object does not get re-created. To do so, we place a $memo$ call around the call to $split$ in $quicksortInternal$, with the head of the list to split as argument. Keyed allocation is not possible to apply here, due to the reasons explained above. 

For our memoized implementation, we observe that updates in the beginning of the input list cause a dramatic change of the DDG structure. Informally speaking, the reason is that the first split will execute differently, thus changing the call tree of the whole program. For updates not at the beginning of the list, however, we observe that only subtrees or parts of subtrees have to be re-executed. 

A plot of pure trace distance for the memoized version of quicksort can be seen in figure \ref{plot:memo_quicksort_pure}. The result is, again, similar to the result given by allocation-sensitive trace distance for the unmemoized version. 

\begin{figure}
  \centering
  \begin{tikzpicture}
  \begin{axis}[xlabel=$i$,ylabel=$u$,zlabel=$\delta_a$]
  \addplot3[surf]
  table{data/QuicksortAlloc.txt};
  \end{axis}
  \end{tikzpicture}
  \caption[margin=5]{Allocation sensitive trace distance for updates of a quicksort}
  \label{plot:quicksort_alloc}
\end{figure}

\begin{figure}
  \centering
  \begin{tikzpicture}
  \begin{axis}[xlabel=$i$,ylabel=$u$,zlabel=$\delta_p$]
  \addplot3[surf]
  table{data/MemoQuicksortPure.txt};
  \end{axis}
  \end{tikzpicture}
  \caption[margin=5]{Pure trace distance for updates of a memoized version of quicksort}
  \label{plot:memo_quicksort_pure}
\end{figure}

\chapter{Conclusion}

During this writing, we explained the basic concepts of self-adjusting computation and provided an introduction to the TBD platform, that enables programmers to create self-adjusting programs. We also explained how the API of TBD is structured and how programs on top of TBD have to be written, including key elements like modifiables and explicit memoization. 

Then, we expanded the DDG concept to match TBD, and defined properties a node in a DDG must have so we can calculate trace distance between two DDGs. We also introduced so-called node tags that are used to compare nodes when calculating trace distance. Using node tags, we created three definitions of equality, that resulted in meaningful definitions of trace distance: 

\begin{itemize}
\item \textit{Pure Trace Distance}, that counts all nodes that were actually re-executed during change propagation and therefore provides a metric about how well a program performed during change propagation. 
\item \textit{Memo-Sensitive Trace Distance}, that only counts nodes that had different parameters and therefore had to be re-executed during change propagation, providing hints about how to accomplish memoization. 
\item \textit{Allocation-Sensitive Trace Distance}, that only counts nodes that had different parameters and also ignores all references to modifiables when comparing traces. Allocatin-sensitive trace distance therefore provides a metric about how well the memoized program would perform if all memory is re-used in a way that prevents additional change propagation. 
\end{itemize} 

We then discussed how we implemented this functionality and explained how we overcame challenges like checking closures for equality and collecting the necessary data for calculating trace distance. Also, we introduced our testing framework that includes test generators and a visualizer that is capable of comparing DDGs of different executions. 

Finally, we evaluated pure trace distance and allocation-sensitive trace distance by comparing pure and allocation-sensitive trace distance of unmemoized programs and pure trace distance for memoized programs with keyed allocation. We showed that the test results were equal to the results we expected, and especially, that the results for allocation-sensitive trace distance for the unmemoized version of each program were equal to the results for pure trace distance for the memoized version with keyed allocation for each program. 

We therefore conclude that using allocation-sensitive trace distance, it is possible to predict how a unmemoized program performs when it is memoized correctly and keyed allocation is added. 

\section{Future Work}

To calculate trace distance, it was necessary to collect a lot of information about the executed program and the DDG. We believe that it is possible to implement automatic optimization based on that information. For instance, we could utilize the output of memo-sensitive trace distance to heuristically memoize nodes after a few runs of a program. Also, automated error checking based on DDGs might be possible. For example, it could be checked whether a memo call includes all variables that are bound from an outer scope to the memo calls closure. If this is not done, the memo call is potentially unsafe since the variable might change but the change is not considered when checking for memo matches. 

Besides that, our framework and our implementation of trace distance can be useful for further analysis of self-adjusting programs. Especially analyzing algorithms with a structure that changes a lot during change propagation, like quicksort, could be a lot easier using our tools. 

%\chapter{Use Cases}
%\label{ch:use_cases}

%\chapter{Discussion}
%\label{ch:discussion}