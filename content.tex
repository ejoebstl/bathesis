
\chapter{Self-Adjusting Programs}
\label{ch:self_adjusting}

When changing the input set of a self-adjusting program, the program has to adjust the internal state and the program output to match the new input. This requires that the internal state of the program has to be known before propagating input changes. The first execution of the program, called \textit{Initial Run} executes the whole program with the given input and captures the program state. The state consists of all data and control dependencies of the execution and all functions called during program executing, including their referencing environment. 
For each successive input update, a so called \textit{Change Propagation Algorithm} consults the program state, finds the parts of the program to re-execute and re-executes them, updating the output and the state.  
This chapter explains which datastructures are used to hold the state and how the mechanisms of change propagation work. The principles of self-adjusting programs using DDGs and memoization were first described by Acar in \cite{Acar2005thesis}, including a theoretical analysis. 

As already mentioned, the a self-adjusting program has no way to modify the output directly. Instead, the change propagation is handled by the platform or language which provides us self-adjusting computation. For our simple map example from section \ref{sec:simple_example}, this means that we do no longer have to write specialized code to find the application of the mapping function and update the output. When using self-adjusting computation, we would just write the program in a suitable language, as we would write a non-incremental version, and the underlying platform will take care about dynamizing the program.

\section{Dynamic Dependence Graphs}

A \textit{Dynamic Dependence Graph} (\textit{DDG}) is a directed, acyclic graph. The nodes of this graph represent the function calls in the program. These nodes also hold the state of the referencing environment, or, in other words, including all parameters of the function and all variables bound from outer scopes. The edges represent control dependencies and data dependencies. 

%Simple Map DDG
\begin{figure}
\begin{center}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,
  thick,call node/.style={circle,fill=white!20,draw},
  data node/.style={rectangle,fill=white!20,draw}]

  \node[data node] (0) {0};
  \node[data node] (1) [right of=0] {1};
  \node[data node] (2) [right of=1] {2};
  \node[data node] (3) [right of=2] {3};

  \node[call node] (4) [below of=0, ultra thick] {map(0)};
  \node[call node] (5) [below of=1] {map(1)};
  \node[call node] (6) [below of=2] {map(2)};
  \node[call node] (7) [below of=3] {map(3)};

  \node[data node] (8) [below of=4] {0};
  \node[data node] (9) [below of=5] {2};
  \node[data node] (10) [below of=6] {4};
  \node[data node] (11) [below of=7] {6};

  %Input next edges
  \path[thin]
    (0) edge node [above] {next} (1)
    (1) edge node [above] {next} (2)
    (2) edge node [above] {next} (3)
    (8) edge node [above] {next} (9)
    (9) edge node [above] {next} (10)
    (10) edge node [above] {next} (11);

  %Read/Write dependencies
  \path[dashed]
    (0) edge node [left] {data} (4)
    (1) edge node [left] {data} (5)
    (2) edge node [left] {data} (6)
    (3) edge node [left] {data} (7)
    (4) edge node [left] {data} (8)
    (5) edge node [left] {data} (9)
    (6) edge node [left] {data} (10)
    (7) edge node [left] {data} (11);

  %Controld ependencies
  \path[ultra thick]
    (4) edge node [above] {call} (5)
    (5) edge node [above] {call} (6)
    (6) edge node [above] {call} (7);
\end{tikzpicture}
\end{center}
\caption{The DDG of a map operation with four list elements}
\label{fig:map_ddg}
\end{figure}

Figure \ref{fig:map_ddg} shows the DDG of a single execution of an recursive implementation of our map-sample from Section \ref{sec:simple_example} with four input elements. The program consists of a single function $map$ which reads the element, applies the mapping function $f$ to the value of the element and calls $map$ again with the elements successor. The square nodes at the top denote elements of the input list. The square nodes at the bottom represent elements of the output list. The round nodes represent function calls to the function $map$. 

The thin edges labeled with \textit{next} indicate the next pointers of the input and output list's nodes. Those edges are not a part of the DDG. The dashed edges labeled with \textit{data} correspond to data dependencies. Note that there is a \textit{read} dependency from each input element to the corresponding application of the mapping function $f$ and from there to the corresponding output element. The thick edges labeled with \textit{call} correspond to the control dependencies. Those control dependencies reflect that $map$ is recursively called for each element. 

On an input change, the change propagation would now track all outgoing dependencies from the changed input values and re-execute the nodes which are dependent on the changed value. During this process, it is possible that values which affect other parts of the program are updated. Those values are then queued for change propagation. Also, since new function calls might be placed during change propagation, the program structure and therefore the DDG can change.  

%Simple Map DDG with change propagation
\begin{figure}
\begin{center}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2cm,
  thick,call node/.style={circle,fill=white!20,draw},
  data node/.style={rectangle,fill=white!20,draw}]

  \node[data node] (0) {0};
  \node[data node] (1) [right of=0, style={draw=red}] {4};
  \node[data node] (2) [right of=1] {2};
  \node[data node] (3) [right of=2] {3};

  \node[call node] (4) [below of=0, ultra thick] {map(0)};
  \node[call node] (5) [below of=1, style={draw=red}] {map(4)};
  \node[call node] (6) [below of=2, style={draw=red}] {map(2)};
  \node[call node] (7) [below of=3, style={draw=red}] {map(3)};

  \node[data node] (8) [below of=4] {0};
  \node[data node] (9) [below of=5, style={draw=red}] {8};
  \node[data node] (10) [below of=6, style={draw=red}] {4};
  \node[data node] (11) [below of=7, style={draw=red}] {6};

  %Input next edges
  \path[thin]
    (0) edge node [above] {} (1)
    (1) edge node [above] {} (2)
    (2) edge node [above] {} (3)
    (8) edge node [above] {} (9)
    (9) edge node [above] {} (10)
    (10) edge node [above] {} (11);

  %Read/Write dependencies
  \path[dashed]
    (0) edge node [left] {} (4)
    (3) edge node [left] {} (7)
    (4) edge node [left] {} (8);

  \path[red, dashed, ultra thick]
    (1) edge node [left] {} (5)
    (2) edge node [left] {} (6)
    (5) edge node [left] {} (9)
    (6) edge node [left] {} (10)
    (7) edge node [left] {} (11);

  %Controld ependencies
  \path[ultra thick]
    (4) edge node [above] {} (5)
    (5) edge node [above] {} (6);

  \path[red, ultra thick]
    (6) edge node [above] {} (7);
\end{tikzpicture}
\end{center}
\caption{The DDG of map after change propagation}
\label{fig:map_change_ddg}
\end{figure}

Figure \ref{fig:map_change_ddg} shows the DDG of the simple map example after changing the second input value from $1$ to $4$. The highlighted edges have been traversed, and the highlighted nodes were therefore updated. It can be seen that the nodes $map(2)$ and $map(3)$ have been re-executed. The re-execution was not necessary, since the parameters and therefore the result of the function call were the same before change propagation. For a long list, this would lead to unnecessarily re-evaluating a lot of function calls. The outcome of such a behavior is linear time complexity in the worst case, even if only a single element has been updated. To circumvent this shortcoming, another technique is required: \textit{Memoization}.

\section{Memoization}

\textit{Memoization}, or also called \textit{Function Caching} is the task of storing results of expensive computations. Basically, it is possible to remember the input parameters and the result for a given function. Then, the parameters of repetitive calls can be compared to the stored parameters. If the parameters match, the result can be re-used. Such a match is called \textit{memo match}.

In combination with DDGs, this means that we can re-use whole subtrees of the DDG during change propagation. Figure \ref{fig:map_change_ddg_memo} illustrates the change propagation process using a \textit{memoized DDG}. Again, the change propagation algorithm re-evaluates functions dependent on the changed input. When the algorithm reaches the call to $map(2)$ however, the parameters of the call match the previous call. The algorithm therefore skips re-executing the call to $map(2)$ and re-uses the subtree. With this technique, the update of a constant number of elements finishes in constant time, for lists of any length. 
The memo match is highlighted in green. 

%Simple Map DDG with change propagation
\begin{figure}
\begin{center}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2cm,
  thick,call node/.style={circle,fill=white!20,draw},
  data node/.style={rectangle,fill=white!20,draw}]

  \node[data node] (0) {0};
  \node[data node] (1) [right of=0, style={draw=red}] {4};
  \node[data node] (2) [right of=1] {2};
  \node[data node] (3) [right of=2] {3};

  \node[call node] (4) [below of=0, ultra thick] {map(0)};
  \node[call node] (5) [below of=1, style={draw=red}] {map(4)};
  \node[call node] (6) [below of=2, style={draw=green}] {map(2)};
  \node[call node] (7) [below of=3] {map(3)};

  \node[data node] (8) [below of=4] {0};
  \node[data node] (9) [below of=5, style={draw=red}] {8};
  \node[data node] (10) [below of=6] {4};
  \node[data node] (11) [below of=7] {6};

  %Input next edges
  \path[thin]
    (0) edge node [above] {} (1)
    (1) edge node [above] {} (2)
    (2) edge node [above] {} (3)
    (8) edge node [above] {} (9)
    (9) edge node [above] {} (10)
    (10) edge node [above] {} (11);

  %Read/Write dependencies
  \path[dashed]
    (0) edge node [left] {} (4)
    (2) edge node [left] {} (6)
    (3) edge node [left] {} (7)
    (4) edge node [left] {} (8)
    (6) edge node [left] {} (10)
    (7) edge node [left] {} (11);

  \path[red, dashed, ultra thick]
    (1) edge node [left] {} (5)
    (5) edge node [left] {} (9);

  %Control ependencies
  \path[ultra thick]
    (4) edge node [above] {} (5)
    (6) edge node [above] {} (7);

  \path[red, ultra thick]
    (5) edge node [above] {} (6);
\end{tikzpicture}
\end{center}
\caption{The memoized DDG of map after change propagation}
\label{fig:map_change_ddg_memo}
\end{figure}

It should be noted that for memoization to work, all functions are required to be \textit{pure} or \textit{side-effect free}. 

\section{Trace Distance}
\label{sec:trace_distance}

Since each node of a DDG hold all information to uniquely identify a function call, the DDG can be used to compare two executions. For each pair of executions $(E, F)$, we count all the nodes which are in the DDG of $E$ but not in the DDG of $F$ and vice versa. The sum of those counts is called \textit{Trace Distance}. Trace distance provides an indicator about how much two executions of a program differ. During a change propagation, at least those differences have to be eliminated by re-executing the corresponding nodes. Therefore, the trace distance of two executions provides a lower bound for change propagation time between those executions, which is a very useful feature. 

Calculating trace distance is simple: For two executions $(E, F)$, all nodes of $E$ which have an equivalent node in $F$ can be found using a greedy matching. The trace distance between the executions is the count of all other nodes. 

If we consult our simple map example with two executions which only differ in one input element, the trace distance will be exactly one, since only a single node will be re-evaluated. This is also illustrated in Figure \ref{fig:map_change_ddg_memo}.

It is important to realize that trace distance is independent of memoization. The reason for this is that function calls which are not memoized can still be equal, due to having equal function arguments. Therefore, the memoized and unmemoized versions of map have equal trace distance for each pair of executions, even though change propagation times differ heavily.    


%[Todo: Maybe two small graphs here, with diffs highlighted.]

\section{A more complex Example}

As already mentioned, the example from Section \ref{sec:simple_example} does not have dependencies between different function calls, and is therefore straight forward to dynamize. We therefore conclude this section with a more complex example. 

Many programming languages feature a function which is capable of combining all elements of a list with some associative operator, until there is only a single result left. This operation is known as \textit{reduce}, \textit{combine} or \textit{fold}.  

\subsection{Naive Approach}

Again, we consider a linked list $A = (a_1, ..., a_n)$ with list nodes $a_i = (v_i, n_i)$, each having a value and a next node, and a combining function $f(v_x, v_y) = r$, where $v_x$ and $v_y$ are input values and $r$ denotes the result. $v_x$, $v_y$ and $r$ are of the same type. Further, we consider the list nodes immutable for our program only. Changes can be done from outside the program when the input changes.

A classic reduce operation would read the first two nodes $a_1$ and $a_2$ in the list, combine the value $v_1$ with the value $v_2$ and remember the result $f(v_1, v_2) = r_1$. Then, the next node $a_3$ is read, the value is combined with the previous result $r_1$ and the result $r_2$ is stored again. In general we can say that for each node $a_i$ we combine the value of the node $v_i$ with the previous result $r_{i-1}$. The last result $r_{n-1}$ is the final result of the function. 

If we now track dependencies for this operation, each result $r_i$ depends on the previous result $r_{i - 1}$ or the initial value $r_0$. Each result $r_i$ also depends on the value $v_i$ of the corresponding node $a_i$. In other words, the fold operation for each node $a_j, j > 1$ depends on the successors of the respective node $a_{j - k}, 0 < k < j$.

The dependency graph of such an implementation is shown in figure \ref{fig:naive_reduce}. 

%Foldl DDG
\begin{figure}
\begin{center}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2cm,
  thick,call node/.style={circle,fill=white!20,draw},
  data node/.style={rectangle,fill=white!20,draw}]

  \node[data node] (0) {0};
  \node[data node] (1) [right of=0] {1};
  \node[data node] (2) [right of=1] {2};
  \node[data node] (3) [right of=2] {3};
  \node[data node] (4) [right of=3] {4};
  \node[data node] (5) [right of=4] {5};
  \node[data node] (6) [right of=5] {6};
  \node[data node] (7) [right of=6] {7};

  \node[call node] (10) [below of=1, ultra thick] {f(0, 1)};
  \node[call node] (11) [below of=2] {f(1, 2)};
  \node[call node] (12) [below of=3] {f(3, 3)};
  \node[call node] (13) [below of=4] {f(6, 4)};
  \node[call node] (14) [below of=5] {f(10, 5)};
  \node[call node] (15) [below of=6] {f(15, 6)};
  \node[call node] (16) [below of=7] {f(21, 7)};

  \node[data node] (18) [below of=16] {28};

  %Input next edges
  \path[thin]
    (0) edge node [above] {} (1)
    (1) edge node [above] {} (2)
    (2) edge node [above] {} (3)
    (3) edge node [above] {} (4)
    (4) edge node [above] {} (5)
    (5) edge node [above] {} (6)
    (6) edge node [above] {} (7);

  %Read/Write dependencies
  \path[dashed]
    (0) edge node [left] {} (10)
    (1) edge node [left] {} (10)
    (2) edge node [left] {} (11)
    (3) edge node [left] {} (12)
    (4) edge node [left] {} (13)
    (5) edge node [left] {} (14)
    (6) edge node [left] {} (15)
    (7) edge node [left] {} (16)

    (16) edge node [left] {} (18)

    (10) edge [bend right] node {} (11)
    (11) edge [bend right] node {} (12)
    (12) edge [bend right] node {} (13)
    (13) edge [bend right] node {} (14)
    (14) edge [bend right] node {} (15)
    (15) edge [bend right] node {} (16);

  %Control ependencies
  \path[ultra thick]
    (10) edge node [left] {} (11)
    (11) edge node [left] {} (12)
    (12) edge node [left] {} (13)
    (13) edge node [left] {} (14)
    (14) edge node [left] {} (15)
    (15) edge node [left] {} (16);

\end{tikzpicture}
\end{center}
\caption{DDG of a naive reduce operation}
\label{fig:naive_reduce}
\end{figure}

If now a change to the first node in the input happens, a change propagation always has to re-execute the fold operation for the whole list, since every single intermediate result and the final result depend on the first node of the list. For this case, the asymptotic complexity of change propagation lies within $O(n)$, where $n$ donates the input since. This expected bound also holds for the average case. The algorithm therefore updates in $O(n)$, which is not better than the expected time for a re-execution. 

Note that memoization is of no help here, since there are data dependencies from each function call to the result of the previous reduction. Every function call depends on all it's predecessors in the DDG.

\subsection{Tree-Like Implementation}

Combining all elements in a list can also be done in a \textit{tree-like} manner. This means that instead of combining all elements from the beginning to the end of the list, elements are pairwise combined, until the list is empty. 

More formally, for our input list $A_1 = (a_{1, 1}, a_{1, 2}, ..., a_{1, n})$, we combine each element with an odd index $a_i$ with its successor $a_{i + 1}$, and concatenate the results to a new list $A_2 = (a_{(2, 1)}, ..., a_{2, k})$. If the input list has an odd number of elements, the last element of the input list is added to the result list. Then, we repeat this operation with the result list, until there is only a single element left.

During each reduction step, half of the list is eliminated. That implies that for $n$ input elements, the count of reduction steps lies within the complexity class $O(log(n))$. If we consult the data dependencies, each element in the list $A_j, j > 1$ directly depends on two elements, which are contained in the list $A_{j - i}$. Since there are only $O(log(n))$ reduction steps and therefore only $O(log(n))$ intermediate lists, the length of the path of dependencies from the input to each DDG node, including the final result, lies within $O(log(n))$. It is therefore possible to propagate the change of a constant number of values in $O(log(n)$. 

%Tree-Fold DDG
\begin{figure}
\begin{center}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2cm,
  thick,call node/.style={circle,fill=white!20,draw},
  data node/.style={rectangle,fill=white!20,draw}]

  \node[data node] (0) {0};
  \node[data node] (1) [right of=0] {1};
  \node[data node] (2) [right of=1] {2};
  \node[data node] (3) [right of=2] {3};
  \node[data node] (4) [right of=3] {4};
  \node[data node] (5) [right of=4] {5};
  \node[data node] (6) [right of=5] {6};
  \node[data node] (7) [right of=6] {7};

  \node[call node] (10) [below of=0, ultra thick] {f(0, 1)};
  \node[call node] (11) [below of=2] {f(2, 3)};
  \node[call node] (12) [below of=4] {f(4, 5)};
  \node[call node] (13) [below of=6] {f(6, 7)};

  \node[call node] (14) [below of=10] {f(1, 5)};
  \node[call node] (15) [below of=12] {f(9, 13)};

  \node[call node] (16) [below of=14] {f(6, 22)};

  \node[data node] (18) [below of=16] {28};

  %Input next edges
  \path[thin]
    (0) edge node [above] {} (1)
    (1) edge node [above] {} (2)
    (2) edge node [above] {} (3)
    (3) edge node [above] {} (4)
    (4) edge node [above] {} (5)
    (5) edge node [above] {} (6)
    (6) edge node [above] {} (7);

  %Read/Write dependencies
  \path[dashed]
    (0) edge node [left] {} (10)
    (1) edge node [left] {} (10)
    (2) edge node [left] {} (11)
    (3) edge node [left] {} (11)
    (4) edge node [left] {} (12)
    (5) edge node [left] {} (12)
    (6) edge node [left] {} (13)
    (7) edge node [left] {} (13)

    (10) edge node [left] {} (14)
    (11) edge node [left] {} (14)
    (12) edge node [left] {} (15)
    (13) edge node [left] {} (15)

    (14) edge node [left] {} (16)
    (15) edge node [left] {} (16)

    (16) edge node [left] {} (18);

  %Control ependencies
  \path[ultra thick]
    (10) edge node [left] {} (11)
    (11) edge node [left] {} (12)
    (12) edge node [left] {} (13)
    (13) edge node [left] {} (14)
    (14) edge node [left] {} (15)
    (15) edge node [left] {} (16);

\end{tikzpicture}
\end{center}
\caption{DDG of a tree-like reduce operation}
\label{fig:tree_reduce}
\end{figure}

Figure \ref{fig:tree_reduce} shows the DDG of the tree-like reduce operation. It can be recognized that there are no data dependencies, which connect all nodes, like before. The control dependencies, while still existent, do not bear any problems, since we can utilize memoization to stop change propagation between nodes which feature no data dependencies. 

While the upper bound of $O(log(n))$ holds for updates of values, this bound is not guaranteed for inserts or deletions of whole list elements from the input list. If we remove or add a single element, we change the index of all successive elements. Since we combine each odd element with its successor, changing the index of elements also changes which elements are combined. If now, for example, the first element is removed, we have to re-evaluate the reduction of all successors, which equals an re-evaluation for the entire list. 
It is possible to circumvent this drawback by using a slightly more complicated approach which involves randomization. This approach is discussed in section \ref{ch:use_cases}. 

\chapter{The TBD Platform}
\label{ch:tbd_platform}

\textit{TBD}, short for \textit{To Be Determined} is a platform for self-adjusting computation. TBD was developed in 2014 at Carnegie Mellon University, the sourcode and documentation of TBD can be found at \url{https://github.com/twmarshall/tbd/}. TBD is implemented in \textit{Scala}, an object-functional language implemented on top of the \textit{Java Virtual Machine}. All programs written on top of TBD are self-adjusting and therefore incremental. For calculating trace distance, the implementation of the underlying platform is of importance, since it influences how programs are structured and how DDG nodes represent function calls. 

It is important to note that all functions in programs written on top of TBD have to be side-effect free and deterministic. This means that no function is allowed to write global variables and that two function calls with the same arguments must produce the same outcome. If this is not respected by a program, change propagation will fail, since the corresponding dependencies can not be tracked. 

This section discusses how TBD provides self-adjusting computation, how variables are represented, and especially how programs on top of TBD are written. 

\section{Modifiables}

%[Maybe move this chapter to Self-Adjusting computing]

For accomplishing self-adjusting computation, it is necassary to detect when a memory locations is read or written. With a classic machine model which features random access memory, reads and writes can be hard to track. Furthermore, high-level languages like Scala usually do not provide data types to reference memory locations. 

Therfore, all variables which need to be tracked are wrapped into so called \textit{Modifiables}, which are typed references to some storage location. Each modifiable has a unique identifier which can be used to reference the modifiable through the program. All modifiables have to be explecitly allocated, read and written by calling correspnding functions. It is therefore possible to recognize dependencies between allocation, write and read of a modifiable. 

\subsection{Keyed Allocation}
\label{sec:keyed_alloc}

In general the allocation of a modifiable creates a new modifiable with a new, unique id. If we, however, read a modifiable we created ourselves later during execution, we have to update all calls which pass the modifiable to where it is finally read. The reason for this is that a new mod has just been introduced and has to replace the old mod through the computation. Since reads and writes to modifiables can be tracked, this is unnecassary, since the change propagation mechanism would take care of updating the calculation where possible. 

To illustrate this problem, consider a simple recursive list reverse operation. This operation takes an input list $A = (a_1, ..., a_n)$ and creates the output list $A_r = (a_n, ..., a_1)$. When we implement such an operation recursively, we usually read the list from front to end and prepend each element to an accumulator list, which holds the reversed $A'$ list in the end. Whenever we prepend an element to the accumulator list, however we have to create a new modifiable to hold the new list element. This list element serves as the head $k_n$ of the accumulator list for the next step.  

%Reverse DDG
\begin{figure}
\begin{center}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2cm,
  thick,call node/.style={circle,fill=white!20,draw},
  data node/.style={rectangle,fill=white!20,draw}]

  \node[data node] (0) {0};
  \node[data node] (1) [right of=0] {1};
  \node[data node] (2) [right of=1] {2};
  \node[data node] (3) [right of=2] {3};
  \node[data node] (4) [right of=3] {4};
  \node[data node] (5) [right of=4] {5};
  \node[data node] (6) [right of=5] {6};
  \node[data node] (7) [right of=6] {7};

  \node[call node] (9) [below of=0, ultra thick] {$r(0, -)$};
  \node[call node] (10) [below of=1] {$r(1, k_0)$};
  \node[call node] (11) [below of=2] {$r(2, k_1)$};
  \node[call node] (12) [below of=3] {$r(3, k_2)$};
  \node[call node] (13) [below of=4] {$r(4, k_3)$};
  \node[call node] (14) [below of=5] {$r(5, k_4)$};
  \node[call node] (15) [below of=6] {$r(6, k_5)$};
  \node[call node] (16) [below of=7] {$r(7, k_6)$};

  \node[data node] (18) [below of=16] {7};
  \node[data node] (19) [below of=15] {6};
  \node[data node] (20) [below of=14] {5};
  \node[data node] (21) [below of=13] {4};
  \node[data node] (22) [below of=12] {3};
  \node[data node] (23) [below of=11] {2};
  \node[data node] (24) [below of=10] {1};
  \node[data node] (25) [below of=9] {0};

  %Input/Output next edges
  \path[thin]
    (0) edge node [above] {} (1)
    (1) edge node [above] {} (2)
    (2) edge node [above] {} (3)
    (3) edge node [above] {} (4)
    (4) edge node [above] {} (5)
    (5) edge node [above] {} (6)
    (6) edge node [above] {} (7)

    (18) edge node [above] {} (19)
    (19) edge node [above] {} (20)
    (20) edge node [above] {} (21)
    (21) edge node [above] {} (22)
    (22) edge node [above] {} (23)
    (23) edge node [above] {} (24)
    (24) edge node [above] {} (25);

  %Read/Write dependencies
  \path[dashed]
    (0) edge node [left] {} (9)
    (1) edge node [left] {} (10)
    (2) edge node [left] {} (11)
    (3) edge node [left] {} (12)
    (4) edge node [left] {} (13)
    (5) edge node [left] {} (14)
    (6) edge node [left] {} (15)
    (7) edge node [left] {} (16)

    (16) edge node [left] {} (18)
    (15) edge node [left] {} (19)
    (14) edge node [left] {} (20)
    (13) edge node [left] {} (21)
    (12) edge node [left] {} (22)
    (11) edge node [left] {} (23)
    (10) edge node [left] {} (24)
    (9) edge node [left] {} (25)

    (9) edge [bend right] node {} (10)
    (10) edge [bend right] node {} (11)
    (11) edge [bend right] node {} (12)
    (12) edge [bend right] node {} (13)
    (13) edge [bend right] node {} (14)
    (14) edge [bend right] node {} (15)
    (15) edge [bend right] node {} (16);

  %Control ependencies
  \path[ultra thick]
    (9) edge node [left] {} (10)
    (10) edge node [left] {} (11)
    (11) edge node [left] {} (12)
    (12) edge node [left] {} (13)
    (13) edge node [left] {} (14)
    (14) edge node [left] {} (15)
    (15) edge node [left] {} (16);

\end{tikzpicture}
\end{center}
\caption{DDG of a reverse operation}
\label{fig:reverse_ddg}
\end{figure}

When we update a value in our input list, we have to re-execute the creation of the corresponding list element in the output list. Since this changes the modifiable which is now the head of the accumulator list, we also have to re-execute the next call, and so on. This causes a worst-case change propagation time of $O(n)$, even with constant sized updates. This DDG of such a function is shown in Figure \ref{fig:reverse_ddg}. The recursive reverse call is donated as $r$, with the element as first, and the head of the accumulator list $k_n$ as second parameter. The data dependencies between the reverse calls arise from the next-relation of the list elements and the allocated memory location for the accumulator list. 

The solution for this problem is called \textit{Keyed Allocation}. When a modifiable is allocated using a key which has already been used for creating a modifiable before, the old memory location is re-used, resulting in an equal mod. In our reverse example, we would use the next element as key for allocating the accumulator. Since the resulting modifiable and therefore the head of the accumulator list will be the same as before change propagation, a memo match occours and the next call is not called by the change propagation algorithm. Using this approach, constant change propagation time is reached for constant sized updates. 

\section{Programming Interface}

As described in the previous section, the usage of a special memory model enables TBD to track read-write dependencies. When such a dependency is detected, however, the code which depends on the changed variable has to be identified and re-evaluated. This requires that the environment of the previous execution is still available. For many programming languages, this task is not straight-forward. Scala, however, supports the usage of \textit{Closures}. Closures are functions which are paired with their enclosing environment. This means that a closure can hold all information necassary to repeat a function valuation. Furthermore, closures can be treated and passed around like variables. Therfore, TBD adds a closure to each read, write or allocation call as additional parameter.

%For each each allocation or read of a modifiable and each explicit memoization call, TBD requires a closure as additional argument. For memoization or allocation, the closure must not have any parameters. For an allocation, only code executed from within the closure may write to the allocated memory location. Memoization operations simply memoize the return value of the closure given as argument. A list of values which is passed to the memoization call is used to determine memo matches. 
%For a read call, the closure has to accept a parameter, the value which has been read. Whenever the value which was read has changed and call has to be re-evaluated, the corresponding closure is re-executed with the new value as parameter.

\subsection{API Functions provided by TBD} 

Regarding TBD, modifiables are instances of the polymorph class $Mod[T]$, where $T$ is a type parameter which specifies the type of the object stored in the modifiable. TBD provides a range of functions to manipulate modifiables in a certain way to enable self-adjusting computation. These functions are provided by the static class $TBD$, and are available globally for TBD programs. The API is summarized in Figure \ref{??}. 

[Todo: UML Here?]

Mods are allocated by calling the $mod$ function. The $mod$ function requires accepts a closure without any parameters as first argument. The code contained in this closure is responsible for writing to the allocated memory location exactly once, by issuing a $write$ call. The write call must be the last operatione executed within the closure. Additionally, there is an overload of the $mod$ function, which accepts a key of any type as second parameter. This key is used to provide keyed allocation, as described in section \ref{sec:keyed_alloc}. Howerver same key might be used in different parts of the same application, for example when a list is first sorted and then reversed. This would break keyed allocation, since the program would overwrite memory which is allocated by another part of the programe. Therefore, it is possible to create a so called $Modizer$, an object which provides the same functionality as the $mod$ function, but only matches a key with the same memory location if the $Modizer$ instance is also the same. This means that a $Modizer$ can be used to add context to the keyed allocation. [Explain context in this context more.]

To read a mod, the $read$ function has to be used. The $read$ function requires a modifiable as first parameter, and a closure as second paramter. The closure has to take a single parameter, the value of the mod which has been read. The value of the mod should be used by and is only valid within the closure. When the value of the modifiable changes, the closure is re-executed with the new value during change propagation. The last operation in the closure has to be a $write$ operation. Therefore, the only way a $read$ can produce a result 
is to write it to a modifiable. The modifiable has to be allocated by a $mod$ instruction which contains the $read$ somewhere in its closure. 

[Memo]

[Par]

\subsection{Programming patterns for TBD}

\section{DDG format for TBD}

\section{Example}



%\section{Program interface}

\chapter{Calculating Trace Distance}
\label{ch:implementation}

\section{Node Tags}

\section{Trace Distance}
The main problem is that DDG nodes, which represent function calls, have to be compared for equality. The problem of comparing functions in general is, however, one of the unsolved problems of computer science. 

%\chapter{Implementation}
%\label{ch:impl}

%\section{Architecture}

%\section{Data Collection}

%\subsection{Usage of Scala Macros}

%\chapter{Use Cases}
%\label{ch:use_cases}

%\chapter{Discussion}
%\label{ch:discussion}