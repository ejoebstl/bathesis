\section{Introduction}
Classic programs and algorithms work on a fixed input set and produce some kind of output. For input data which changes frequently, a conventional program has to be re-executed frequently too, if the output has to be up-to-date. If the change is very small, but the dataset itself is very big, the re-execution will take a lot of time compared to the significance of the data change. Therefore, approaches to avoid a complete re-execution exist. For example, an algorithm can make use of special data-structures or memorization of intermediate results to adjust the output to match the new input data. The target of such an approach is to create a program which is capable of adopting to input data changes and update the output accordingly, faster than by re-execution. This concept is called \textit{incremental computation} \cite{Ramalingam:IncrementalBibliography}. Programs or algorithms which exploit this concept are called \textit{incremental programs} or \textit{incremental algorithms}. 

The task of re-evaluating certain parts of a program as soon as the input data changes is called \textit{Change Propagation}. The target of this re-evaluation is to update the output to match the output of a complete re-evaluation. A change propagation algorithm is responsible for selecting the function calls in the program, which have to be re-executed. When the program is executed for the first time with a certain input set, no change propagation happens, the program is executed in a conventional way. This execution is called the \textit{Initial Run} \cite{Acar2005thesis}.

\subsection{Approaches to incremental computation}
While it is possible to create incremental programs or data structures which are optimized for a single purpose, for example \textit{dynamic} or \textit{kinetic data structures} \cite{Guibas98kineticdata}. Naturally, these algorithms are complex, single-purpose and can not be composed with other algorithms, which makes them difficult to use in practice \cite{Acar2005thesis}. 
 
A more general appraoch is to provide a platform which natively supports incremental computation for all programs writton on top of that platform. The approach of automatically inferring a incremental program from a non-incremental program is also called \textit{self-adjusting computation} in literature \cite{Harper2004} \cite{Acar2005thesis}. 

Well-known ways of automatically transforming conventional programs into incremental programs are: 
\begin{itemize}
\item Providing a high-level abstraction, like an incremental database. \cite{Peng2010}
\item Deriving an incremental program from a non-incremental, functional program. \cite{liu1995systematic}  \cite{ley2008compiling} 
\item Deriving an incremental program from a non-incremental, non-functional program. \cite{heydon2000caching} \cite{Pugh1989} \cite{cohen1991dynamic} \cite{naiadIncremental} \cite{Hammer2009} \cite{Chen2014} \cite{Acar2008} \cite{acar2006adaptive} 
\end{itemize}

While it has been shown that a incremental program can be derived from a functional program automatically \cite{ley2008compiling}, this problem is not solved for non-functional programs. This especially holds true if the underlying programming framework or API is complex, which is practically the case for every usable modern language. While it is possible to express every program in a functional language, many real-world applications are written in non-functional languages. 
The same argument implies that high-level abstractions, while very useful for certain cases, are also not sufficient in general. 

For deriving incremental programs from non-functional programs the following approaches exist: 
\begin{itemize}
\item Memorization using function call caching. \cite{heydon2000caching} \cite{Pugh1989} 
\item Finding calls to re-evaluate using dependency graphs. \cite{cohen1991dynamic}
\item A combination of multiple approaches, like memorization and dependency graphs. \cite{Hammer2009} \cite{Chen2014} \cite{Acar2008} \cite{acar2006adaptive} \cite{naiadIncremental}
\end{itemize}

While memorization and the utilization of dependency graphs lead to the re-use of some intermediate results, has been shown that these approaches alone do not perform optimal \cite{Acar2005thesis}. Combining these procedures, however, leads to optimal results in terms of change propagation overhead. 

A \textit{Dynamic Dependence Graph (DDG)} can be described as a data structure, which holds a directed graph for tracking control and data dependencies during execution \cite{Acar2005thesis}, whereas the nodes of the graph are function calls in the program. In contrast to \textit{Static Dependence Graphs} \cite{Demers1981}, DDGs are mutated during change propagation and therefore adjusted to the new program structure. 

\textit{Memorization} is the concept of storing intermediate results and re-using them during change propagation. Memorization can be combined with DDGs, by inserting memorization nodes into the graph. During change propagation, this can lead to a significant performance increase, because entire sub-trees of the call graph and their corresponding results can be re-used \cite{Acar2005thesis}.

The work of U. Acar et al describes the theoretical and practical concept of incremental computing using memorization and dynamic dependency graphs in detail \cite{Acar2005thesis}.

\subsection{Designing suitable algorithms}

While the approaches described in the previous section can provide change propagation for any program, the performance of the change propagation strongly depends on the structure of the program and the arising dependencies during an execution. 

For example, consider a naive fold operation on a linked list $A = (a_1, ..., a_n)$ with list nodes $a_i = (v_i, n_i)$, each having a value and a next node, and an intial value $r_0$. We consider the list nodes immutable for our program, however changes can be done from outside the program, for example when the input changes. The fold operation would read the first node $a_1$ in the list, combine the value $v_1$ with the inital value $r_0$ and remember the result $r_1$. Then, the next $a_2$ node is read, the value is combined with the previous result $r_1$ and the result $r_2$ is stored again. 

If we now track dependencies for this operation, each result $r_i$ depends on the previous result $r_{i - 1}$ or the initial value $r_0$. Each result $r_i$ also depends on the value $v_i$ of the corrsponding node $a_i$. In other words, the fold operation for each node $a_j, j > 1$ depends on the successors of the respective node $a_{j - k}, 0 < k < j$.

If now a change to the first node in the input happens, a change propagation always has to re-execute the fold operation for the whole list, since every single intermediate result and the final result depend on the first node of the list. For this case, the asymptotic complexity of change propagation lies within $O(n)$ whereas $n$ donates the input sice. This expected bound also holds for the average case \cite{Acar2005thesis}. The algorithm therefore updates in $O(n)$, which is not better than the expected time for a re-execution. 

If we however use a randomized, tree-like fold, which combines groups of neighbors until the input list consists of only a single element, we can reach a bound of $O(log_k(n))$, whereas $n$ denotes the input size and $k$ the expected group sice. 
The need for randomization arises from the possibility of insertions or deletions in the input data: If we utilize only the position of our list nodes to decide which nodes we combine, an insertion or delition would lead to a change of this decision for each node which is a successor of the inserted or deleted node. A description and detailed analysis of this algorithm is available in \cite{Acar2005thesis}. The mentioned algorithm updates in $O(log(n))$, which makes change propagation more efficient than re-execution. 

These two examples clearly illustrate that a developer has to take care of dependencies when writing programs. An optimal conventional approach might lead to slow change propagation. While this can be straight forward for small problems with few dependencies, keeping track of the dependencies and estimating the performance of a program can become complicated very quickly. This is especially true for algorithms where an input update can change the whole call tree. An example for such an algorithm would be a quicksort\cite{hoare1962quicksort}, where a change of the first pivot element can change the partition of the list and therefore all further subcalls. 

From this examples we conclude that the existence of program analysis tools for incremental algorithms is crucial for progression of incremental computation platforms. 

\subsection{Algorithm Analysis}

As discussed in the previous section, the asymptotic complexity of propagating input changes through the program is of interest. While inferring asymptotic complexity is a task which is usually done by hand for a given algorithm, this approach can be hard for incremental programs, due to the complexity of underlying models and change propagation algorithms. 

From a practical viewpoint however, asymptotic complexity is not always the only important property of a program. For real-world purposes, a benchmark or an analysis of a certain program execution can be sufficient to yield a meaningful statement about program performance \cite{vokolos1998performance}.

Given two execution traces of the same incremental program with different input data, we present a practical approach to calculate a lower bound for change propagation time between these two traces. We can also show that our approach works independently from the change propagation algorithm utilized by the program. While it is not safely possible to infer asymptotic complexity from the obtained data, we can make statements regarding the expected change propagation time for given update sizes. Especially, we can decide whether the change propagation is reasonably faster than a complete re-execution of the program. 

Naturally, the performance of change propagation is not the only interesting property. If the performance is not as good as expected, the question about how to increase performance arises. Since execution traces provide us with detailed sets of control and data dependencies, we can determine and inspect relationships between subsets of the program execution. By utilizing these dependencies, we are able to automatically spot issues in the program, especially regarding the correct use of memorization. Due to the mere count of dependencies which arise even in small programs, this task is cumbersome to be done by hand, while it can be easily automated. 

Exploiting these approaches, we create a tool, which assists developers when writing incremental programs: The tool provides metrics about the performance of an incremental program and also suggests changes of the program structure to increase performance where applicable. 