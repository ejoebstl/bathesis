\section{Fundamental work}
The work of U. Acar et al\cite{Acar2005thesis} describes the theoretical and practical concept of incremental computing using memorization and DDGs in detail. Since these concepts are important for understanding this writing, they are summarized in this section. First, general terms are explained briefly, then, a short theoretical outline for the term of stable algorithms is provided. 

\subsection{Change Propagation, Dynamic Dependence Graphs and Memorization}

A \textit{Dynamic Dependence Graph (DDG)} can be described as a data structure, which holds a directed graph for tracking control and data dependencies during execution \cite{Acar2005thesis}, whereas the nodes of the graph are usually function calls in the program. In contrast to \textit{Static Dependence Graphs} \cite{Demers1981}, DDGs are mutated during change propagation and therefore adjusted to the new program structure. 

\textit{Memorization} is the concept of storing intermediate results and re-using them during change propagation. Memorization can be combined with DDGs, by inserting memorization nodes into the graph. During change propagation, this can lead to a significant performance increase, because entire sub-trees of the call graph and their corresponding results can be re-used \cite{Acar2005thesis}. 

\subsection{Execution Traces}

\label{sec:ddg_memo}
A trace is a theoretical construct which can be described as an ordered tree, whereas nodes represent function calls during the program execution \cite{Acar2005thesis}. While similar to the DDG, a trace tracks no data dependencies. A trace usually resembles the call tree of a program. 

\begin{definition}[General trace node equality]
Each node $v$ is uniquely described by a tag, consisting of

\begin{itemize}
\item the function being called, $fun(v)$
\item the arguments of the function call, $args(v)$
\item the values read in the body of the function, $reads(v)$
\item the values returned to the function from its callees, $returns(v)$
\item the weight of the function, $w(v)$, which is equal to its execution time.
\end{itemize}

Two nodes $v$ and $v'$ are equal, denoted $v \equiv v'$, if $fun(v) = fun(v')$, $args(v) = args(v')$, $reads(v) = reads(v')$ and $returns(v) = returns(v')$.
\end{definition}

\subsection{Trace distance}
Trace distance can basically be described as an edit distance between two execution traces \cite{Acar2005thesis} \cite{acar2004dynamizing}. 

To find the minimum trace distance of two traces $T$ and $T'$, the so called \textit{cognates} relation can be used. 

\begin{definition}[Cognates]
A set of cognates $C$ is a relation of two traces $T$ and $T'$ with the set of nodes $V$ and $V'$, so that

\begin{itemize}
\item $C \subset V \times V'$
\item for each $(v, v') \in V: v \equiv v'$ 
\item no node is paired with more than one node
\end{itemize}
\end{definition}
All nodes of both traces $T$ and $T'$ can be colored either blue, yellow or red. 
Nodes which have a cognate are colored blue. Nodes of $T$ without a cognate are colored yellow. Nodes of $T'$ without a cognate are colored red. 

The trace distance can now be calculated by summing up the weights of all yellow and red nodes. 

\begin{definition}[Trace distance]
The trace distance $\delta(T, T')$ between two traces $T$ and $T'$ is given by
\begin{align*}
	\delta(T, T') = \sum_{y \in Y} w(y) + \sum_{r \in R} w(r)
\end{align*}
whereas $Y$ denotes the set of all yellow vertices and $R$ is the set of all red vertices. 
\end{definition}

If the cognate relation $C$ is maximal, the intrinsic distance is minimal, denoted as $\delta^{min}$. Also, a maximal cognate relation can be found using a naive greedy algorithm \cite{Acar2005thesis} \cite{acar2004dynamizing}.

The minimal trace distance forms a lower bound for the duration of change propagation, since during change propagation all red vertices have to be deleted, and all yellow vertices have to be re-evaluated \cite{Acar2005thesis}. 

\subsection{Stability of algorithms}

When we inspect change propagation not only for a single run, but for all possible runs of an algorithm, we can find an upper bound for the expected time of change propagation. This upper bound, denoted in Landau notation as $O(f(n))$, expresses the expected time for change propagation for a change of a constant number of elements in the input data \cite{Acar2005thesis}. 

If the expected time for change propagation of an algorithm $A$ lies within $O(f(n))$, the algorithm is called $O(f(n))$\textit{-stable}. 
Algorithms which are $O(g(n))$-stable are called \textit{stable algorithms} \cite{Acar2005thesis}, if $g(n)$ is a sub-linear growing function.