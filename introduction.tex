
\chapter{Introduction}
\label{ch:Introduction}
%[Explain Target: Incremental Computation, Change Propagation and Trace Distance] 

Trace distance is an important metric for incremental programs. This metric measures the re-execution cost that occours when the input to an incremental program changes and the program is partially re-executed. The objective of this work is to provide a software tool capable of calculating trace distance for certain incremental programs.  

The term \textit{incremental program} refers to a program that after a change of the program input only re-evaluates the computation steps that depend on the changed parts of the input. Therefore, such a program can adjust the output faster than it would be possible by re-executing the whole program with the new input. In particular, it can is possible to adjust to constant-sized updates in constant time, even if a re-execution would run in polynomial time complexity or worse. The process of adjusting the program output and program state to input changes is called \textit{change propagation}. Incremental programs are also called \textit{dynamic programs} in literature. 

\textit{Trace distance} can be roughly described as metric for the amount of differences in the internal program state between two executions of the same program with differing inputs. Since the changes in program state reflect the re-evaluated computation steps, this number is directly related to the time needed for change propagation. Trace distance can therefore be used as benchmark for incremental programs. 

\section{A simple Incremental Program}
\label{sec:simple_example}
Consider a simple program that takes a list, consisting of nodes $a_0$ to $a_n$ as input. Each list node has a numeric value and a successor. The program reads each node, and applies a mapping function $g$ to the value of the node, and outputs a list containing the results. The functionality of this program is also implemented as \textit{map operator} in many programming languages. For this example, let $g$ be a simple function, only multiplying the value by two. 

Executing this program for a list always has a time complexity of $O(n)$, even if only a single element $a_i$ changes. An incremental version of this program would, however, find the single call $f_i$ that is affected by the update of node $a_i$, re-execute this single call, and update the output accordingly. If an element is inserted into the input, $g$ will also be re-executed, and the new element will be inserted. If an element is removed from the input, the corresponding element will simply be removed from the output. With this approach updating the output has time complexity $O(k)$, where $k$ is the size of the change in the input. This implies that we can update the output in constant time, as long as the changes to the input have constant size. 

Note that this program is particularly easy to incrementalize, since there are no dependencies between different calls to our mapping function $g$. 
\section{Approaches to Incremental Computation}
%Since incremental programs have been subject to research for over 30 years, different approaches exist. 

%[Introduce Terminology: Self-Adjusting Computation]

A well-known form of creating incremental programs is to design an incremental algorithm for a single purpose. Such algorithms are carefully modified to hold enough internal state to make change propagation possible. A simple example for such an algorithm would be a basic insertion-sort algorithm. For insertion-sort, a newly added input value can simply be inserted into the output, a value removed from the input can be removed from the output and an updated input value can be removed and then added again. More sophisticated examples would be so-called \textit{kinetic algorithms} or \textit{kinetic data structures}. Those data structures are for instance used to perform geometric calculations, such as finding bounding boxes or intersection areas, on groups of moving objects, as described in \cite{yu2008practical} and \cite{basch2004kinetic} respectively.

\subsection{Self-Adjusting Computation}

However, in \cite{Acar2005thesis}, Acar pointed out that the utilization of such algorithms can be cumbersome. The development of incremental algorithms might be complex, the algorithms are usually single purpose and they cannot be composed to a bigger applications in general. Therefore, the development of languages or platforms that automate the process of deriving incremental programs from non-incremental programs is of interest. After such a transformation, the program should be capable of adjusting its internal state and the program output to reflect changes of the program input. This particular approach to incremental computation is called \textit{self-adjusting computation}.

For accomplishing self-adjusting computation, tracking control and data dependencies through the program and storing intermediate results can be used. These techniques are called \textit{dynamic dependence graphs (DDGs)} and \textit{memoization}, respectively. 

In 2014, an self-adjusting computation platform called \textit{TBD} was developed at Carnegie Mellon University. TBD is an abbreviation for \textit{To Be Determined}. The platform is implemented in the \textit{Scala} language and makes use of DDGs and memoization to accomplish self-adjusting computation. 

\section{Contents of this Work}

This work focuses on the development of a tool that is capable of calculating trace distance for programs implemented on top of TBD. First, the principles of self-adjusting computation will be explained in depth, especially the process change propagation using DDGs and memoization. Second, the program interface of the TBD platform will be described. Third, our approach of calculating trace-distance will be discussed. Then, we discuss our implementation including the framework for data collection using Scala macros. 
Finally, we will show how our tool can be used for analyzing algorithms by discussing the implementation of a $map$, a $reduce$ and a $sort$ function.  